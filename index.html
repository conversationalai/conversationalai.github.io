<!DOCTYPE html>
<!-- saved from url=(0026)https://ai.googleblog.com/ -->
<html class="v2 list-page" dir="ltr" itemscope="" itemtype="http://schema.org/Blog" lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:b="http://www.google.com/2005/gml/b" xmlns:data="http://www.google.com/2005/gml/data" xmlns:expr="http://www.google.com/2005/gml/expr"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link href="./blog_files/2437439463-css_bundle_v2.css" rel="stylesheet" type="text/css">
<title>
Converse AI
</title>
<meta content="IE=Edge" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0, height=device-height">
    <title>ConverseAI</title>
      <meta property="og:image" content="http://localhost/static/images/share.png">
    <meta property="og:title" content="ConversationalAI">
    <meta property="og:site_name" content="ConversationalAI">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost/">
    <meta name="description" content="At Conversational AI, our aim is to create intelligent and meaningful dialogue agents that help make people's lives easier and bring about social good.">
    <meta property="og:description" content="At Conversational AI, our aim is to create intelligent and meaningful dialogue agents that help make people's lives easier and bring about social good.">
    <link rel="shortcut icon" href="./system/files/conversationalAI-logo.png" type="image/x-icon">

    <link href="./index_files/main.min.css" rel="stylesheet">
    <link href="./index_files/css" media="all" rel="stylesheet">
    <!-- Google Tag Manager -->
    <script type="text/javascript" async="" src="./index_files/analytics.js"></script><script async="" src="./index_files/gtm.js"></script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-MTDXW5P');</script>
    <!-- End Google Tag Manager -->
    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/angular.min.js"></script>
  </head>
  <body class="page--home">
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MTDXW5P"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->


<meta content="width=device-width, height=device-height, minimum-scale=1.0, initial-scale=1.0, user-scalable=0" name="viewport">
<meta content="IE=Edge" http-equiv="X-UA-Compatible">
<meta content="Converse AI" property="og:title">
<meta content="en_US" property="og:locale">
<meta content="https://conversationalai.github.io" property="og:url">
<meta content="Converse AI" property="og:site_name">
<!-- Twitter Card properties -->
<meta content="Converse AI" property="og:title">
<meta content="summary" name="twitter:card">
<meta content="@Communicate_AI" name="twitter:creator">
<meta name="description" content="At Conversational AI, our aim is to create intelligent and meaningful dialogue agents that help make people's lives easier and bring about social good.">
<meta property="og:description" content="At Conversational AI, our aim is to create intelligent and meaningful dialogue agents that help make people's lives easier and bring about social good.">
<link href="./blog_files/css" rel="stylesheet" type="text/css">
<link href="./blog_files/icon" rel="stylesheet">
<script src="./blog_files/cb=gapi.loaded_2" async=""></script><script src="./blog_files/cb=gapi.loaded_1" async=""></script><script src="./blog_files/cb=gapi(1).loaded_0" async=""></script><script async="" src="./blog_files/analytics.js"></script><script src="./blog_files/jquery.min.js" type="text/javascript"></script>
<!-- End -->
<style id="page-skin-1" type="text/css"><!--
/*
<Group description="Header Color" selector="header">
<Variable name="header.background.color" description="Header Background"
type="color" default="#ffffff"/>
</Group>
*/
.header-outer {
border-bottom: 1px solid #e0e0e0;
background: #ffffff;
}
html, .Label h2, #sidebar .rss a, .BlogArchive h2, .FollowByEmail h2.title, .widget .post h2 {
font-family: Roboto, sans-serif;
}
.plusfollowers h2.title, .post h2.title, .widget h2.title {
font-family: Roboto, sans-serif;
}
.widget-item-control {
height: 100%;
}
.widget.Header, #header {
position: relative;
height: 100%;
width: 100%;
}
}
.widget.Header .header-logo1 {
float: left;
margin-right: 15px;
padding-right: 15px;
border-right: 1px solid #ddd;
}
.header-title h1 {
color: rgba(39, 37, 37, 0.69);
display: inline-block;
font-size: 36px;
font-family: Roboto, sans-serif;
font-weight: normal;
line-height: 50px;
vertical-align: top;
margin-left: -23px;
}
.header-inner {
background-repeat: no-repeat;
background-position: right 0px;
}
.post-author,
.byline-author {
font-size: 14px;
font-weight: normal;
color: #757575;
color: rgba(0,0,0,.54);
}
.post-content .img-border {
border: 1px solid rgb(235, 235, 235);
padding: 4px;
}
.header-title a {
text-decoration: none !important;
}
pre {
border: 1px solid #bbbbbb;
margin-top: 1em 0 0 0;
padding: 0.99em;
overflow-x: auto;
overflow-y: auto;
}
pre, code {
font-size: 9pt;
background-color: #fafafa;
line-height: 125%;
font-family: monospace;
}
pre, code {
color: #060;
font: 13px/1.54 "courier new",courier,monospace;
}
.header-left .header-logo1 {
width: 128px !important;
}
.header-desc {
line-height: 20px;
margin-top: -22px;
margin-left: 17px;
}
.fb-custom img, .twitter-custom img, .gplus-share img {
cursor: pointer;
opacity: 0.54;
}
.fb-custom img:hover, .twitter-custom img:hover, .gplus-share img:hover {
opacity: 0.87;
}
.fb-like {
width: 80px;
}
.post .share {
float: right;
}
#twitter-share{
border: #CCC solid 1px;
border-radius: 3px;
background-image: -webkit-linear-gradient(top,#ffffff,#dedede);
}
.twitter-follow {
background: url(//3.bp.blogspot.com/-M7uPAxKEeh4/WKrvV1ujKCI/AAAAAAAATZE/cdHhTldtvk4q4ad1Me1XDIgQD9Aul09CACK4B/s1600/twitter-bird.png) no-repeat left center;
padding-left: 18px;
font: normal normal normal 11px/18px 'Helvetica Neue',Arial,sans-serif;
font-weight: bold;
text-shadow: 0 1px 0 rgba(255,255,255,.5);
cursor: pointer;
margin-bottom: 10px;
}
.twitter-fb {
padding-top: 2px;
}
.fb-follow-button  {
background: -webkit-linear-gradient(#4c69ba, #3b55a0);
background: -moz-linear-gradient(#4c69ba, #3b55a0);
background: linear-gradient(#4c69ba, #3b55a0);
border-radius: 2px;
height: 18px;
padding: 4px 0 0 3px;
width: 57px;
border: #4c69ba solid 1px;
}
.fb-follow-button a {
text-decoration: none !important;
text-shadow: 0 -1px 0 #354c8c;
text-align: center;
white-space: nowrap;
font-size: 11px;
color: white;
vertical-align: top;
}
.fb-follow-button a:visited {
color: white;
}
.fb-follow {
padding: 0px 5px 3px 0px;
width: 14px;
vertical-align: bottom;
}
.gplus-wrapper {
margin-top: 3px;
display: inline-block;
vertical-align: top;
}
.twitter-custom, .gplus-share {
margin-right: 12px;
}
.fb-follow-button{
margin: 10px auto;
}
sub, sup {
line-height: 0;
}
/** CUSTOM CODE **/
.post-content td {
width: inherit;
}
--></style>
<style id="template-skin-1" type="text/css"><!--
.header-outer {
clear: both;
}
.header-inner {
margin: auto;
padding: 0px;
}
.footer-outer {
background: #f5f5f5;
clear: both;
margin: 0;
}
.footer-inner {
margin: auto;
padding: 0px;
}
.footer-inner-2 {
/* Account for right hand column elasticity. */
max-width: calc(100% - 248px);
}
.google-footer-outer {
clear: both;
}
.cols-wrapper, .google-footer-outer, .footer-inner, .header-inner {
max-width: 978px;
margin-left: auto;
margin-right: auto;
}
.cols-wrapper {
margin: auto;
clear: both;
margin-top: 60px;
margin-bottom: 60px;
overflow: hidden;
}
.col-main-wrapper {
float: left;
width: 100%;
}
.col-main {
margin-right: 278px;
max-width: 660px;
}
.col-right {
float: right;
width: 248px;
margin-left: -278px;
}
/* Tweaks for layout mode. */
body#layout .google-footer-outer {
display: none;
}
body#layout .header-outer, body#layout .footer-outer {
background: none;
}
body#layout .header-inner {
height: initial;
}
body#layout .cols-wrapper {
margin-top: initial;
margin-bottom: initial;
}
--></style>
<!-- start all head -->

<meta content="blogger" name="generator">
<link href="https://ai.googleblog.com/favicon.ico" rel="icon" type="image/x-icon">
<link href="http://ai.googleblog.com/" rel="canonical">
<link rel="alternate" type="application/atom+xml" title="Google AI Blog - Atom" href="http://ai.googleblog.com/feeds/posts/default">
<link rel="alternate" type="application/rss+xml" title="Google AI Blog - RSS" href="http://ai.googleblog.com/feeds/posts/default?alt=rss">
<link rel="service.post" type="application/atom+xml" title="Google AI Blog - Atom" href="https://www.blogger.com/feeds/8474926331452026626/posts/default">
<link rel="openid.server" href="https://www.blogger.com/openid-server.g">
<link rel="openid.delegate" href="http://ai.googleblog.com/">
<!--[if IE]><script type="text/javascript" src="https://www.blogger.com/static/v1/jsbin/3658603751-ieretrofit.js"></script>
<![endif]-->
<meta content="http://ai.googleblog.com/" property="og:url">
<meta content="Google AI Blog" property="og:title">
<meta content="The latest news from Google AI" property="og:description">
<!--[if IE]> <script> (function() { var html5 = ("abbr,article,aside,audio,canvas,datalist,details," + "figure,footer,header,hgroup,mark,menu,meter,nav,output," + "progress,section,time,video").split(','); for (var i = 0; i < html5.length; i++) { document.createElement(html5[i]); } try { document.execCommand('BackgroundImageCache', false, true); } catch(e) {} })(); </script> <![endif]-->
<!-- end all head -->
<!--<base target="_self">--><base href="." target="_self">
<style>
      html {
        font-family: Roboto, sans-serif;
        -moz-osx-font-smoothing: grayscale;
        -webkit-font-smoothing: antialiased;
      }
      body {
        padding: 0;
        /* This ensures that the scroll bar is always present, which is needed */
        /* because content render happens after page load; otherwise the header */
        /* would "bounce" in-between states. */
        min-height: 150%;
      }
      h2 {
        font-size: 16px;
      }
      h1, h2, h3, h4, h5 {
        line-height: 2em;
      }
      html, h4, h5, h6 {
        font-size: 14px;
      }
      a, a:visited {
        color: #4184F3;
        text-decoration: none;
      }
      a:focus, a:hover, a:active {
        text-decoration: none;
      }
      .Header {
        margin-top: 15px;
      }
      /*.Header h1 {
        font-size: 32px;
        font-weight: 300;
        line-height: 32px;
        height: 42px;
      }*/
      .header-inner .Header .titlewrapper {
        padding: 0;
        margin-top: 30px;
      }
      .header-inner .Header .descriptionwrapper {
        padding: 0;
        margin: 0;
      }
      .cols-wrapper {
        margin-top: 56px;
      }
      .header-outer, .cols-wrapper, .footer-outer, .google-footer-outer {
        padding: 0 60px;
      }
      .header-inner {
        height: 256px;
        position: relative;
      }
      html, .header-inner a {
        color: #212121;
        color: rgba(0,0,0,.87);
      }
      .header-inner .google-logo {
        display: inline-block;
        background-size: contain;
        z-index: 1;
        height: 100px;
        overflow: hidden;
        margin-top: -23px;
        margin-right: 8px;
      }
      .header-left {
        position: absolute;
        top: 50%;
        -webkit-transform: translateY(-50%);
        transform: translateY(-50%);
        margin-top: 2px;
        width: 100%;
        margin-left: -15px;
      }
      .google-logo {
        margin-left: -4px;
      }
      .google-logo img{
        height: 250px;
    	margin-top: -79px;
      }
      #google-footer {
        position: relative;
        font-size: 13px;
        list-style: none;
        text-align: right;
      }
      #google-footer a {
        color: #444;
      }
      #google-footer ul {
        margin: 0;
        padding: 0;
        height: 144px;
        line-height: 144px;
      }
      #google-footer ul li {
        display: inline;
      }
      #google-footer ul li:before {
        color: #999;
        content: "\00b7";
        font-weight: bold;
        margin: 5px;
      }
      #google-footer ul li:first-child:before {
        content: '';
      }
      #google-footer .google-logo-dark {
        left: 0;
        margin-top: -16px;
        position: absolute;
        top: 50%;
      }
      /** Sitemap links. **/
      .footer-inner-2 {
        font-size: 14px;
        padding-top: 42px;
        padding-bottom: 74px;
      }
      .footer-inner-2 .HTML h2 {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-size: 14px;
        font-weight: 500;
        padding-left: 0;
        margin: 10px 0;
      }
      .footer-inner-2 .HTML ul {
        font-weight: normal;
        list-style: none;
        padding-left: 0;
      }
      .footer-inner-2 .HTML li {
        line-height: 24px;
        padding: 0;
      }
      .footer-inner-2 li a {
        color: rgba(65,132,243,.87);
      }
      /** Archive widget. **/
      .BlogArchive {
        font-size: 13px;
        font-weight: normal;
      }
      .BlogArchive .widget-content {
        display: none;
      }
      .BlogArchive h2, .Label h2 {
        color: #4184F3;
        text-decoration: none;
      }
      .BlogArchive .hierarchy li {
        display: inline-block;
      }
      /* Specificity needed here to override widget CSS defaults. */
      .BlogArchive #ArchiveList ul li, .BlogArchive #ArchiveList ul ul li {
        margin: 0;
        padding-left: 0;
        text-indent: 0;
      }
      .BlogArchive .intervalToggle {
        cursor: pointer;
      }
      .BlogArchive .expanded .intervalToggle .new-toggle {
        -ms-transform: rotate(180deg);
        transform: rotate(180deg);
      }
      .BlogArchive .new-toggle {
        float: right;
        padding-top: 3px;
        opacity: 0.87;
      }
      #ArchiveList {
        text-transform: uppercase;
      }
      #ArchiveList .expanded > ul:last-child {
        margin-bottom: 16px;
      }
      #ArchiveList .archivedate {
        width: 100%;
      }
      /* Months */
      .BlogArchive .items {
        max-width: 150px;
        margin-left: -4px;
      }
      .BlogArchive .expanded .items {
        margin-bottom: 10px;
        overflow: hidden;
      }
      .BlogArchive .items > ul {
        float: left;
        height: 32px;
      }
      .BlogArchive .items a {
        padding: 0 4px;
      }
      .Label {
        font-size: 13px;
        font-weight: normal;
      }
      .sidebar-icon {
        display: inline-block;
        width: 24px;
        height: 24px;
        vertical-align: middle;
        margin-right: 12px;
        margin-top: -1px
      }
      .Label a {
        margin-right: 4px;
      }
      .Label .widget-content {
        display: none;
      }
      .FollowByEmail {
        font-size: 13px;
        font-weight: normal;
      }
      .FollowByEmail h2 {
        background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAALCAYAAACZIGYHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAUBJREFUeNrMkSGLAlEUhb+ZB4JFi8mx2cz+ApvhRUGTcUCrNqNJDYIi+DO0GUwmQXDK2DSIoGgZcSaIjDrzwrK4ssvChj1w0733O+fdp+m6PozH4yQSCfb7Pa7r8pOi0SjJZBLP8zgej4gAIMvlMuPxmADIYrHger1+C6lUKmo+NJ/NZojb7SZDWiwWo1qtks1msW2bw+HwZdkwDHq9HvV6nel0SqvVYrvdIh6Ph3Qch+VyqRYLhQJSSjRNw7IsfN9XgGKxSLfbJZfL0e/3aTabrFYr7vc7IujLcOh8PqunrNdr0uk0pVKJVCpFJBJRgEajweVyod1uMxgM2O12BAGUgRbU8DV2JpOhVquRz+cRQii3+XxOp9NRN3jVR5LPOp1OjEYjlSL8hclkgmmabDabt4d+m+S30vkD/R/IU4ABAPTZgnZdmG/PAAAAAElFTkSuQmCC");
        background-repeat: no-repeat;
        background-position: 0 50%;
        text-indent: 30px;
      }
      .FollowByEmail .widget-content {
        display: none;
      }
      .searchBox input {
        border: 1px solid #eee;
        color: #212121;
        color: rgba(0,0,0,.87);
        font-size: 14px;
        padding: 8px 8px 8px 40px;
        width: 164px;
        font-family: Roboto, sans-serif;
        background: url("https://www.gstatic.com/images/icons/material/system/1x/search_grey600_24dp.png") 8px center no-repeat;
      }
      .searchBox ::-webkit-input-placeholder { /* WebKit, Blink, Edge */
        color:    rgba(0,0,0,.54);
      }
      .searchBox :-moz-placeholder { /* Mozilla Firefox 4 to 18 */
        color:    #000;
        opacity:  0.54;
      }
      .searchBox ::-moz-placeholder { /* Mozilla Firefox 19+ */
        color:    #000;
        opacity:  0.54;
      }
      .searchBox :-ms-input-placeholder { /* Internet Explorer 10-11 */
        color:    #757575;
      }
      .widget-item-control {
        margin-top: 0px;
      }
      .section {
        margin: 0;
        padding: 0;
      }
      #sidebar-top {
        border: 1px solid #eee;
      }
      #sidebar-top > div {
        margin: 16px 0;
      }
      .widget ul {
        line-height: 1.6;
      }
      /*main post*/
      .post {
        margin-bottom:30px;
      }
      #main .post .title {
        margin: 0;
      }
      #main .post .title a {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-weight: normal;
        font-size: 24px;
      }
      #main .post .title a:hover {
        text-decoration:none;
        color:#4184F3;
      }
      .message,  #main .post .post-header {
        margin: 0;
        padding: 0;
      }
      #main .post .post-header .caption, #main .post .post-header .labels-caption,  #main .post .post-footer .caption, #main .post .post-footer .labels-caption {
        color: #444;
        font-weight: 500;
      }
      #main .tr-caption-container td {
        text-align: center;
      }
      #main .post .tr-caption {
        color: #757575;
        color: rgba(0,0,0,.54);
        display: block;
        padding-bottom: 20px;
		line-height: 1.5;
      }
      #main .post .tr-caption-container {
        line-height: 24px;
        padding: 4px 0;
        text-align: center;
      }
      #main .post .post-header .published{
        font-size:11px;
        font-weight:bold;
      }
      .post-header .publishdate {
        font-size: 17px;
        font-weight:normal;
        color: #757575;
        color: rgba(0,0,0,.54);
      }
      #main .post .post-footer{
        font-size:12px;
        padding-bottom: 21px;
      }
      .label-footer {
        margin-bottom: 12px;
        margin-top: 12px;
      }
      .comment-img {
        margin-right: 16px;
        opacity: 0.54;
        vertical-align: middle;
      }
      #main .post .post-header .published {
        margin-bottom: 10px;
        margin-top: -2px;
      }
      .post .post-content {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-size: 15px;
        margin: 5px 0 36px 0;
        line-height: 20px;
      }
      .post-body .post-content ul, .post-body .post-content ol {
        margin: 16px 0;
        padding: 0 48px;
      }
      .post-summary {
        display: none;
      }
      /* Another old-style caption. */
      .post-content div i, .post-content div + i {
        font-size: 14px;
        font-style: normal;
        color: #757575;
        color: rgba(0,0,0,.54);
        display: block;
        line-height: 24px;
        margin-bottom: 16px;
        text-align: left;
      }
      /* Another old-style caption (with link) */
      .post-content a > i {
        color: #4184F3 !important;
      }
      /* Old-style captions for images. */
      .post-content .separator + div:not(.separator) {
        margin-top: -16px;
      }
      /* Capture section headers. */
      .post-content br + br + b, .post-content .space + .space + b, .post-content .separator + b {
        display: inline-block;
      }
      /*.post-content li {
        line-height: 1.5;
      }*/
      /* Override all post images/videos to left align. */
      .post-content .separator, .post-content > div {
        text-align: center;
      }
      .post-content .separator > a, .post-content .separator > span {
        margin-left: 0 !important;
      }
      .post-content img {
        max-width: 100%;
        height: auto;
        width: auto;
      }
      .post-content .tr-caption-container img {
        margin-bottom: 12px;
      }
      .post-content iframe, .post-content embed {
        max-width: 100%;
      }
      .post-content .carousel-container {
        margin-bottom: 48px;
      }
      #main .post-content b {
        font-weight: 500;
      }
      /* These are the main paragraph spacing tweaks. */
      #main .post-content br {
        /*content: ' ';*/
        display: block;
        padding: 4px;
      }
      .post-content .space {
        display: block;
        height: 8px;
      }
      .post-content iframe + .space, .post-content iframe + br {
        padding: 0 !important;
      }
      #main .post .jump-link {
        margin-bottom:10px;
      }
      .post-content img, .post-content iframe {
        margin: 15px 0 20px 0;
      }
      .post-content > img:first-child, .post-content > iframe:first-child {
        margin-top: 0;
      }
      .col-right .section {
        padding: 0 16px;
      }
      #aside {
        background:#fff;
        border:1px solid #eee;
        border-top: 0;
      }
      #aside .widget {
        margin:0;
      }
      #aside .widget h2, #ArchiveList .toggle + a.post-count-link {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-weight: 400 !important;
        margin: 0;
      }
      #ArchiveList .toggle {
        float: right;
      }
      #ArchiveList .toggle .material-icons {
        padding-top: 4px;
      }
      #sidebar .tab {
        cursor: pointer;
      }
      #sidebar .tab .arrow {
        display: inline-block;
        float: right;
      }
      #sidebar .tab .icon {
        display: inline-block;
        vertical-align: top;
        height: 24px;
        width: 24px;
        margin-right: 13px;
        margin-left: -1px;
        margin-top: 1px;
        color: #757575;
        color: rgba(0,0,0,.54);
      }
      #sidebar .widget-content > :first-child {
        padding-top: 8px;
      }
      #sidebar .active .tab .arrow {
        -ms-transform: rotate(180deg);
        transform: rotate(180deg);
      }
      #sidebar .arrow {
        color: #757575;
        color: rgba(0,0,0,.54);
      }
      #sidebar .widget h2 {
        font-size: 14px;
        line-height: 24px;
        display: inline-block;
      }
      #sidebar .widget .BlogArchive {
        padding-bottom: 8px;
      }
      #sidebar .widget {
        border-bottom: 1px solid #eee;
        box-shadow: 0px 1px 0 white;
        margin-bottom: 0;
        padding: 14px 0;
        min-height: 20px;
      }
      #sidebar .widget:last-child {
        border-bottom: none;
        box-shadow: none;
        margin-bottom: 0;
      }
      #sidebar ul {
        margin: 0;
        padding: 0;
      }
      #sidebar ul li {
        list-style:none;
        padding:0;
      }
      #sidebar ul li a {
        line-height: 32px;
      }
      #sidebar .archive {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAYCAYAAADzoH0MAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAE1JREFUeNpiNDY23s9AAWBioBCwYBM8c+YMVsUmJibEGYBNMS5DaeMFfDYSZQA2v9I3FrB5AZeriI4FmnrBccCT8mhmGs1MwyAzAQQYAKEWG9zm9QFEAAAAAElFTkSuQmCC");
        height: 24px;
        line-height: 24px;
        padding-left: 30px;
      }
      #sidebar .labels {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAARCAYAAAA7bUf6AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAUxJREFUeNpiNDY23s9AAMycOfM7UF05kHkZmzwTMkdSUhKrIcXFxZy3bt3qBjIN8RrS09PDsHnzZjCNDr58+cKQlpbGDjSoHcg1w2oIyAUODg5gARCNzUVIBrUCuVYYhjx//pzhwIEDYAEQDeJjA1CDWIAGNQK59jBxRuSABbkAlwHIgIeHh2HWrFn/1NTU2oDcvSgBS4wBSC5iArqoCsj1YGIgEyAZVMoEchqlBjEB/cZAiUHg2AEGznpKDAImxOeM////B4VLKtBvEUCngZ1ILKivr3/u6+ubBzJAGZQ9gC5aQoqLgAY8BhkAZL4BuQQkxgXE34A4BuiiZEIuAhrwEGhAEZD5DpzYoIaA2UAM4kQADUrHZRDUgAIg8wO2XAwzbQXQa5OweQ1owB10AyA6gS7BgX1u3ry5397eHow3bdo0EyjGi00tQIABANPgyAH1q1eaAAAAAElFTkSuQmCC");
        height: 20px;
        line-height: 20px;
        padding-left: 30px;
      }
      #sidebar .rss a {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAASCAYAAABWzo5XAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAX5JREFUeNqsVDGSgkAQHL2rIiIikohIc/EBRkbwAIwuwgfwAXiAD9AHSI7kEkECRCb6AIyINDLx7K0aa6kT7uq0q7YYtnZ7umdnt7darXbr9Zpegeu61DNNc0dvwCcH4/GYJpMJnc9nOhwOVJbl/4hAAokMECZJQtvt9k+kH7qufyEYDAakqqqYxFdRFBqNRmTbNg2HQ0rTlK7XayvR0xqBdDqdkuM4dE/0ULhYLOh4PHYrknG5XGi/31MYhuL/nkwonM1mlGUZ1XXdrsiyLGEDhY7juJEZ1u5tIixDGdYhmYw+B7CAzPP5nDabjdgIAgCksMX1832/3drtdqPT6SQWapomiGEFNkDEdpDMMAzK81ys/7XYy+XyoQgq2WoURSIJ2iIIgp/WZCCTvFm2wgeAU31aI3Q2GhIDMeB53qPYPIcm5VrxXIOIOxsDMStjVawAc1VViRgN22lNBiuQN3GR+SY07hpOoStmFQAKXRRFY93bnpG+fONfedi+BRgAbkS8Fxp7QQIAAAAASUVORK5CYII=");
      }
      #sidebar .subscription a {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAALCAYAAACZIGYHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAUBJREFUeNrMkSGLAlEUhb+ZB4JFi8mx2cz+ApvhRUGTcUCrNqNJDYIi+DO0GUwmQXDK2DSIoGgZcSaIjDrzwrK4ssvChj1w0733O+fdp+m6PozH4yQSCfb7Pa7r8pOi0SjJZBLP8zgej4gAIMvlMuPxmADIYrHger1+C6lUKmo+NJ/NZojb7SZDWiwWo1qtks1msW2bw+HwZdkwDHq9HvV6nel0SqvVYrvdIh6Ph3Qch+VyqRYLhQJSSjRNw7IsfN9XgGKxSLfbJZfL0e/3aTabrFYr7vc7IujLcOh8PqunrNdr0uk0pVKJVCpFJBJRgEajweVyod1uMxgM2O12BAGUgRbU8DV2JpOhVquRz+cRQii3+XxOp9NRN3jVR5LPOp1OjEYjlSL8hclkgmmabDabt4d+m+S30vkD/R/IU4ABAPTZgnZdmG/PAAAAAElFTkSuQmCC");
      }
      #sidebar-bottom {
        background: #f5f5f5;
        border-top:1px solid #eee;
      }
      #sidebar-bottom .widget {
        border-bottom: 1px solid #e0e0e0;
        padding: 15px 0;
        text-align: center;
      }
      #sidebar-bottom > div:last-child {
        border-bottom: 0;
      }
      #sidebar-bottom .text {
        line-height: 20px;
      }
      /* Home, forward, and backward pagination. */
      .blog-pager {
        border-top : 1px #e0e0e0 solid;
        padding-top: 10px;
        margin-top: 15px;
        text-align: right !important;
      }
      #blog-pager {
        margin-botom: 0;
        margin-top: -14px;
        padding: 16px 0 0 0;
      }
      #blog-pager a {
        display: inline-block;
      }
      .blog-pager i.disabled {
        opacity: 0.2 !important;
      }
      .blog-pager i {
        color: black;
        margin-left: 16px;
        opacity: 0.54;
      }
      .blog-pager i:hover, .blog-pager i:active {
        opacity: 0.87;
      }
      #blog-pager-older-link, #blog-pager-newer-link {
        float: none;
      }
      .gplus-profile {
        background-color: #fafafa;
        border: 1px solid #eee;
        overflow: hidden;
        width: 212px;
      }
      .gplus-profile-inner {
        margin-left: -1px;
        margin-top: -1px;
      }
      /* Sidebar follow buttons. */
      .followgooglewrapper {
        padding: 12px 0 0 0;
      }
      .loading {
        visibility: hidden;
      }
      .detail-page .post-footer .cmt_iframe_holder {
        padding-top: 40px !important;
      }
      /** Desktop **/
      @media (max-width: 900px) {
        .col-right {
          display: none;
        }
        .col-main {
          margin-right: 0;
          min-width: initial;
        }
        .footer-outer {
          display: none;
        }
        .cols-wrapper {
          min-width: initial;
        }
        .google-footer-outer {
          background-color: #f5f5f5;
        }
      }
      /** Tablet **/
      @media (max-width: 712px) {
        .header-outer, .cols-wrapper, .footer-outer, .google-footer-outer {
          padding: 0 40px;
        }
      }
      /* An extra breakpoint accommodating for long blog titles. */
      @media (max-width: 600px) {
        .header-left {
          height: 100%;
          top: inherit;
          margin-top: 0;
          -webkit-transform: initial;
          transform: initial;
        }
        .header-title {
          margin-top: 88px;
        }
        .header-inner .google-logo {
          height: 40px;
          margin-top: 3px;
        }
        .header-inner .google-logo img {
          height: 200px;
        }
        .header-title h2 {
          font-size: 32px;
          line-height: 40px;
        }
		.header-title h1{
          font-size: 32px;
          line-height: 50px;
		}
        .header-desc {
          bottom: 94px;
          position: absolute;
          margin-left: 14px;
        }
      }
      /** Mobile/small desktop window; also landscape. **/
      @media (max-width: 480px), (max-height: 480px) {
        .header-outer, .cols-wrapper, .footer-outer, .google-footer-outer {
          padding: 0 16px;
        }
        .cols-wrapper {
          margin-top: 0;
        }
        .post-header .publishdate, .post .post-content {
          font-size: 16px;
        }
        .post .post-content {
          line-height: 28px;
          margin-bottom: 30px;
        }
        .post {
          margin-top: 30px;
        }
        .byline-author {
          display: block;
          font-size: 12px;
          line-height: 24px;
          margin-top: 6px;
        }
        #main .post .title a {
          font-weight: 500;
          color: #4c4c4c;
          color: rgba(0,0,0,.70);
        }
        #main .post .post-header {
          padding-bottom: 12px;
        }
        #main .post .post-header .published {
          margin-bottom: -8px;
          margin-top: 3px;
        }
        .post .read-more {
          display: block;
          margin-top: 14px;
        }
        .post .tr-caption {
          font-size: 12px;
        }
        #main .post .title a {
          font-size: 20px;
          line-height: 30px;
        }
        .post-content iframe {
          /* iframe won't keep aspect ratio when scaled down. */
          max-height: 240px;
        }
        .post-content .separator img, .post-content .tr-caption-container img, .post-content iframe {
          max-width: inherit;
          width: calc(100% + 32px);
        }
        /*.post-content table, .post-content td {
          width: 100%;
        }*/
        #blog-pager {
          margin: 0;
          padding: 16px 0;
        }
        /** List page tweaks. **/
        .list-page .post-original {
          display: none;
        }
        .list-page .post-summary {
          display: block;
        }
        .list-page .comment-container {
          display: none;
        } 
        .list-page #blog-pager {
          padding-top: 0;
          border: 0;
          margin-top: -8px;
        }
        .list-page .label-footer {
          display: none;
        }
        .list-page #main .post .post-footer {
          border-bottom: 1px solid #eee;
          margin: -16px 0 0 0;
          padding: 0 0 20px 0;
        }
        .list-page .post .share {
          display: none;
        }
        /** Detail page tweaks. **/
        .detail-page .post-footer .cmt_iframe_holder {
          padding-top: 32px !important;
        }
        .detail-page .label-footer {
          margin-bottom: 0;
        }
        .detail-page #main .post .post-footer {
          padding-bottom: 0;
        }
        .detail-page #comments {
          display: none;
        }
      }
      [data-about-pullquote], [data-is-preview], [data-about-syndication] {
        display: none;
      }
    </style>
<noscript>
<style>
        .loading { visibility: visible }</style>
</noscript>
<script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-961555-69', 'auto', 'blogger');
        ga('blogger.send', 'pageview');
      </script>
<link href="./blog_files/authorization.css" media="all" onload="if(media!=&#39;all&#39;)media=&#39;all&#39;" rel="stylesheet"><noscript><link href='https://www.blogger.com/dyn-css/authorization.css?targetBlogID=8474926331452026626&amp;zx=63841e0d-4abc-42b4-981d-614c875380e4' rel='stylesheet'/></noscript>

</head>
<body>
<script type="text/javascript">
      //<![CDATA[
      var axel = Math.random() + "";
      var a = axel * 10000000000000;
      document.write('<iframe src="https://2542116.fls.doubleclick.net/activityi;src=2542116;type=gblog;cat=googl0;ord=ord=' + a + '?" width="1" height="1" frameborder="0" style="display:none"></iframe>');
      //]]>
    </script><iframe src="./blog_files/activityi.html" width="1" height="1" frameborder="0" style="display:none"></iframe>
<noscript>
<img alt='' height='1' src='https://ad.doubleclick.net/ddm/activity/src=2542116;type=gblog;cat=googl0;ord=1?' width='1'/>
</noscript>
<!-- Header -->


<header ng-controller="HeaderController as headerCtrl" class="ng-scope">
  <div class="header__top">
    <a href="https://ai.google/" class="header__hamburger" ng-click="headerCtrl.open()" aria-expanded="false" aria-label="Open the navigation drawer" aria-controls="header__nav">
      <div class="header__hamburger-burger"></div>
    </a>

    <a href="https://conversationalai.github.io" class="header__lockup">
      <div class="header__logo">
        <img src="./system/files/ConverseAI-logo.png" alt="ConverseAI" height="29" width="142" draggable="false">
      </div>
      <!-- <div class="header__product">AI</div> -->
    </a>

    <nav class="header__nav" id="header__nav" aria-label="Navigation">
      <a href="https://ai.google/" class="header__lockup">
        <div class="header__logo">
          <svg>
            <path fill="#4285F4" d="M9.24 8.19v2.46h5.88c-.18 1.38-.64 2.39-1.34 3.1-.86.86-2.2 1.8-4.54 1.8-3.62 0-6.45-2.92-6.45-6.54s2.83-6.54 6.45-6.54c1.95 0 3.38.77 4.43 1.76L15.4 2.5C13.94 1.08 11.98 0 9.24 0 4.28 0 .11 4.04.11 9s4.17 9 9.13 9c2.68 0 4.7-.88 6.28-2.52 1.62-1.62 2.13-3.91 2.13-5.75 0-.57-.04-1.1-.13-1.54H9.24z"></path>
            <path fill="#EA4335" d="M25 6.19c-3.21 0-5.83 2.44-5.83 5.81 0 3.34 2.62 5.81 5.83 5.81s5.83-2.46 5.83-5.81c0-3.37-2.62-5.81-5.83-5.81zm0 9.33c-1.76 0-3.28-1.45-3.28-3.52 0-2.09 1.52-3.52 3.28-3.52s3.28 1.43 3.28 3.52c0 2.07-1.52 3.52-3.28 3.52z"></path>
            <path fill="#4285F4" d="M53.58 7.49h-.09c-.57-.68-1.67-1.3-3.06-1.3C47.53 6.19 45 8.72 45 12c0 3.26 2.53 5.81 5.43 5.81 1.39 0 2.49-.62 3.06-1.32h.09v.81c0 2.22-1.19 3.41-3.1 3.41-1.56 0-2.53-1.12-2.93-2.07l-2.22.92c.64 1.54 2.33 3.43 5.15 3.43 2.99 0 5.52-1.76 5.52-6.05V6.49h-2.42v1zm-2.93 8.03c-1.76 0-3.1-1.5-3.1-3.52 0-2.05 1.34-3.52 3.1-3.52 1.74 0 3.1 1.5 3.1 3.54.01 2.03-1.36 3.5-3.1 3.5z"></path>
            <path fill="#FBBC05" d="M38 6.19c-3.21 0-5.83 2.44-5.83 5.81 0 3.34 2.62 5.81 5.83 5.81s5.83-2.46 5.83-5.81c0-3.37-2.62-5.81-5.83-5.81zm0 9.33c-1.76 0-3.28-1.45-3.28-3.52 0-2.09 1.52-3.52 3.28-3.52s3.28 1.43 3.28 3.52c0 2.07-1.52 3.52-3.28 3.52z"></path>
            <path fill="#34A853" d="M58 .24h2.51v17.57H58z"></path>
            <path fill="#EA4335" d="M68.26 15.52c-1.3 0-2.22-.59-2.82-1.76l7.77-3.21-.26-.66c-.48-1.3-1.96-3.7-4.97-3.7-2.99 0-5.48 2.35-5.48 5.81 0 3.26 2.46 5.81 5.76 5.81 2.66 0 4.2-1.63 4.84-2.57l-1.98-1.32c-.66.96-1.56 1.6-2.86 1.6zm-.18-7.15c1.03 0 1.91.53 2.2 1.28l-5.25 2.17c0-2.44 1.73-3.45 3.05-3.45z"></path>
          </svg>
        </div>
        <div class="header__product">AI</div>
      </a>

          <!--<div class="header__item" aria-level="1">
            <a href="https://ai.google/about/" class="header__link">
              About
            </a>

          </div>
          <div class="header__item" aria-level="1">
            <a href="https://ai.google/stories/" class="header__link">
              Stories
            </a>

          </div>
          <div class="header__item" aria-level="1">
            <a href="https://ai.google/research/" class="header__link">
              Research
            </a>

          </div>
          <div class="header__item" aria-level="1">
            <a href="https://ai.google/education/" class="header__link">
              Education
            </a>

          </div>
          <div class="header__item" aria-level="1">
            <a href="https://ai.google/tools/" class="header__link">
              Tools
            </a>

          </div>
      <div class="header__item" aria-level="1">
        <a href="https://ai.googleblog.com/" class="header__link" target="_blank">
          Blog
        </a>
      </div> -->
    </nav>

    <div class="header__overlay" ng-click="headerCtrl.close()"></div>
  </div>
</header>



</a>
<a href="https://conversationalai.github.io">
<!--<h1>OK</h1>-->
</a>
</div>
<div class="header-desc">
The state-of-the-art in Conversational AI
</div>
</div>
</div></div>
</div>
</div>

<!-- all content wrapper start -->
<div class="cols-wrapper">
<div class="col-main-wrapper">
<div class="col-main">
<div class="section" id="main"><div class="widget Blog" data-version="1" id="Blog1">
<div class="post" data-id="1422353966457667553" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html" itemprop="url" title="Advances in Semantic Textual Similarity">
Advances in Conversational AI
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Wednesday, May 30, 2018
</span>
</div>
</div>
<div class="post-body">
<!--<div class="post-content post-summary"><span class="byline-author">Posted by Yinfei Yang, Software Engineer and Chris Tar,  Engineering Manager, Google AI</span><br>
 <br>
The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as ...<a href="http://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html" itemprop="url" title="Advances in Semantic Textual Similarity" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Yinfei Yang, Software Engineer and Chris Tar,  Engineering Manager, Google AI</span><br>-->
<br>
The state-of-the-art in Conversational AI. Coming soon by May 31st, 2018.
<!-- The recent rapid progress of neural network-based natural language understanding research, especially on learning semantic text representations, can enable truly novel products such as <a href="https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html">Smart Compose</a> and <a href="https://ai.googleblog.com/2018/04/introducing-semantic-experiences-with.html">Talk to Books</a>. It can also help improve performance on a variety of natural language tasks which have limited amounts of training data, such as building strong text classifiers from as few as 100 labeled examples.  <br>
<br>
Below, we discuss two papers reporting recent progress on semantic representation research at Google, as well as two new models available for download on <a href="https://www.tensorflow.org/hub/">TensorFlow Hub</a> that we hope developers will use to build new and exciting applications.<br>
<br>
<b>Semantic Textual Similarity</b><br>
In “<a href="https://arxiv.org/abs/1804.07754">Learning Semantic Textual Similarity from Conversations</a>”, we introduce a new way to learn sentence representations for semantic textual similarity.  The intuition is that sentences are semantically similar if they have a similar distribution of responses. For example, “How old are you?” and “What is your age?” are both questions about age, which can be answered by similar responses such as “I am 20 years old”. In contrast, while “How are you?” and “How old are you?” contain almost identical words, they have very different meanings and lead to different responses.<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-w2kAi39zPrE/Wv2OPHTwDgI/AAAAAAAACvY/aQzvBcaIqYkw8McCBcXlTx0pj9FbILH0ACLcBGAs/s1600/image4.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="618" data-original-width="1294" height="190" src="./blog_files/image4.png" width="400"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Sentences are semantically similar if they can be answered by the same responses. Otherwise, they are semantically different.</td></tr>
</tbody></table>
In this work, we aim to learn semantic similarity by way of a response classification task: given a conversational input, we wish to classify the correct response from a batch of randomly selected responses. But, the ultimate goal is to learn a model that can return encodings representing a variety of natural language relationships, including similarity and relatedness. By adding another prediction task (In this case, the <a href="https://nlp.stanford.edu/projects/snli/">SNLI</a> <a href="https://en.wikipedia.org/wiki/Entailment_(linguistics)">entailment</a> dataset) and forcing both through shared encoding layers, we get even better performance on similarity measures such as the <a href="http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark">STSBenchmark</a> (a sentence similarity benchmark) and <a href="http://alt.qcri.org/semeval2017/task3/">CQA task B</a> (a question/question similarity task). This is because logical entailment is quite different from simple equivalence and provides more signal for learning complex semantic representations. <br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://3.bp.blogspot.com/-qcqYQcxfLS0/Wv2Pxmm945I/AAAAAAAACvk/decC5VtlRGUdD4NqCui3HgNd3LXdjEvlgCLcBGAs/s1600/image3.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="298" data-original-width="899" height="211" src="./blog_files/image3.gif" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">For a given input, classification is considered a ranking problem against potential candidates.</td></tr>
</tbody></table>
<b>Universal Sentence Encoder</b><br>
In “<a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder</a>”, we introduce a model that extends the multitask training described above by adding more tasks, jointly training them with a <a href="https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf">skip-thought</a>-like model that predicts sentences surrounding a given selection of text. However, instead of the encoder-decoder architecture in the original skip-thought model, we make use of an encode-only architecture by way of a shared encoder to drive the prediction tasks. In this way, training time is greatly reduced while preserving the performance on a variety of transfer tasks including sentiment and semantic similarity classification. The aim is to provide a single encoder that can support as wide a variety of applications as possible, including paraphrase detection, relatedness, clustering and custom text classification. <br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-9Qk1fubLpzg/Wv2QGgKVVmI/AAAAAAAACvs/Gm-XF3prXVIIvaIkrTmkcIcYz-4qSxLKwCLcBGAs/s1600/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="586" data-original-width="693" height="337" src="./blog_files/image2.png" width="400"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Pairwise semantic similarity comparison via outputs from TensorFlow Hub Universal Sentence Encoder.</td></tr>
</tbody></table>
As described in our paper, one version of the Universal Sentence Encoder model uses a <a href="https://www.cs.umd.edu/~miyyer/pubs/2015_acl_dan.pdf">deep average network</a> (DAN) encoder, while a second version uses a more complicated self attended network architecture, <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer</a>. <br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://4.bp.blogspot.com/-S0j5RrNgYoc/Wv2QVewdzHI/AAAAAAAACvw/r6t2l3JxoYkLrXTZi9hLSObz3rRzB0UVQCLcBGAs/s1600/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="593" data-original-width="1600" height="235" src="./blog_files/image1.png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Multi-task training as described in “<a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder</a>”. A variety of tasks and task structures are joined by shared encoder layers/parameters (grey boxes).</td></tr>
</tbody></table>
With the more complicated architecture, the model performs better than the simpler DAN model on a variety of sentiment and similarity classification tasks, and for short sentences is only moderately slower. However, compute time for the model using Transformer increases noticeably as sentence length increases, whereas the compute time for the DAN model stays nearly constant as sentence length is increased.<br>
<br>
<b>New Models </b><br>
In addition to the Universal Sentence Encoder <a href="https://tfhub.dev/google/universal-sentence-encoder/1">model</a> described above, we are also sharing two <i>new</i> models on <a href="https://www.tensorflow.org/hub/">TensorFlow Hub</a>:  the <a href="https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder-large/1">Universal Sentence Encoder - Large</a> and <a href="https://www.tensorflow.org/hub/modules/google/universal-sentence-encoder-lite/1">Universal Sentence Encoder - Lite</a>. These are pretrained Tensorflow models that return a semantic encoding for variable-length text inputs. The encodings can be used for semantic similarity measurement, relatedness, classification, or clustering of natural language text.<br>
<ul>
<li>The Large model is trained with the <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer</a> encoder described in our second paper. It targets scenarios requiring high precision semantic representations and the best model performance at the cost of speed &amp; size.</li>
<li>The Lite model is trained on a <a href="https://github.com/google/sentencepiece">Sentence Piece</a> vocabulary instead of words in order to significantly reduce the vocabulary size, which is a major contributor of model size. It targets scenarios where resources like memory and CPU are limited, such as on-device or browser based implementations.</li>
</ul>
We're excited to share this research, and these models, with the community. We believe that what we're showing here is just the beginning, and that there remain important research problems to be addressed, such as extending the techniques to more languages (the models discussed above currently support English). We also hope to further develop this technology so it can understand text at the paragraph or even document level. In achieving these tasks, it may be possible to make an encoder that is truly “universal”.<br>
<br> -->
<!--<b>Acknowledgements</b><br>
<i>Stanford</i>
<span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Advances in Semantic Textual Similarity&amp;url=http://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div> -->
<div class="comment-container">
<i class="comment-img material-icons">
                            👍
                          </i>

</div>
<div class="post-footer">
<div class="cmt_iframe_holder" data-href="http://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html" data-viewtype="FILTERED_POSTMOD" id="undefined"></div>
<a href="https://plus.google.com/112374322230920073195" rel="author" style="display:none;">
                        Google
                      </a>
<div class="label-footer">
<span class="labels-caption">
Labels:
</span>
<span class="labels">
<a class="label" href="" rel="tag">
Deep Learning
</a>

                                ,
                              
<a class="label" href="" rel="tag">
Natural Language Processing
</a>
</span>
</div>
</div>
</div>
<!--<div class="post" data-id="8692394042407275642" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html" itemprop="url" title="Smart Compose: Using Neural Networks to Help Write Emails">
Smart Compose: Using Neural Networks to Help Write Emails
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Wednesday, May 16, 2018
</span>
</div>
</div>
<div class="post-body">
<div class="post-content post-summary"><span class="byline-author">Posted by Yonghui Wu, Principal Engineer, Google Brain Team</span><br>
 <br>
Last week at  <a href="https://events.google.com/io/">Google I/O</a>, we introduced  <a href="https://www.blog.google/products/gmail/subject-write-emails-faster-smart-compose-gmail/">Smart Compose</a>, a new feature in Gmail that uses machine learning to interactively offer sentence completion suggestions as you type, allowing you to draft emails faster. Building upon technology developed for ...<a href="http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html" itemprop="url" title="Smart Compose: Using Neural Networks to Help Write Emails" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Yonghui Wu, Principal Engineer, Google Brain Team</span><br>
<br>
Last week at <a href="https://events.google.com/io/">Google I/O</a>, we introduced <a href="https://www.blog.google/products/gmail/subject-write-emails-faster-smart-compose-gmail/">Smart Compose</a>, a new feature in Gmail that uses machine learning to interactively offer sentence completion suggestions as you type, allowing you to draft emails faster. Building upon technology developed for <a href="https://ai.googleblog.com/2017/05/efficient-smart-reply-now-for-gmail.html">Smart Reply</a>, Smart Compose offers a new way to help you compose messages — whether you are responding to an incoming email or drafting a new one from scratch.<br>
<div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/-KlBuhzV_oFw/WvxP_OAkJ1I/AAAAAAAACu0/T0F6lFZl-2QpS0O7VBMhf8wkUPvnRaPIACLcBGAs/s1600/image2.gif" imageanchor="1" style="margin-left: 1em; margin-right: 1em;" target="_blank"><img border="0" data-original-height="501" data-original-width="700" height="456" src="./blog_files/image2.gif" width="640"></a></div>In developing Smart Compose, there were a number of key challenges to face, including:<br>
<ul><li><b>Latency:</b> Since Smart Compose provides predictions on a per-keystroke basis, it must respond ideally within 100ms for the user not to notice any delays. Balancing model complexity and inference speed was a critical issue.</li>
<li><b>Scale:</b> Gmail is used by more than 1.4 billion diverse users. In order to provide auto completions that are useful for all Gmail users, the model has to have enough modeling capacity so that it is able to make tailored suggestions in subtly different contexts.</li>
<li><b>Fairness and Privacy:</b>  In developing Smart Compose, we needed to address sources of potential bias in the training process, and had to adhere to the same rigorous user privacy standards as Smart Reply, making sure that our models never expose user’s private information. Furthermore, researchers had no access to emails, which meant they had to develop and train a machine learning system to work on a dataset that they themselves cannot read.</li>
</ul><b>Finding the Right Model </b><br>
Typical language generation models, such as <a href="https://en.wikipedia.org/wiki/Language_model">ngram</a>,&nbsp;<a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">neural bag-of-words</a> (BoW) and <a href="https://www.isca-speech.org/archive/interspeech_2010/i10_1045.html">RNN language</a> (RNN-LM) models, learn to predict the next word conditioned on the prefix word sequence. In an email, however, the words a user has typed in the current email composing session is only one “signal” a model can use to predict the next word. In order to incorporate more context about what the user wants to say, our model is also conditioned on the email subject and the previous email body (if the user is replying to an incoming email). <br>
<br>
One approach to include this additional context is to cast the problem as a <a href="https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html">sequence-to-sequence</a> (seq2seq) machine translation task, where the source sequence is the concatenation of the subject and the previous email body (if there is one), and the target sequence is the current email the user is composing. While this approach worked well in terms of prediction quality, it failed to meet our strict latency constraints by orders of magnitude.<br>
<br>
To improve on this, we combined a BoW model with an RNN-LM, which is faster than the seq2seq models with only a slight sacrifice to model prediction quality. In this hybrid approach, we encode the subject and previous email by averaging the <a href="https://www.tensorflow.org/tutorials/word2vec">word embeddings</a> in each field. We then join those averaged embeddings, and feed them to the target sequence RNN-LM at every decoding step, as the model diagram below shows.<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-ilOCekdQP0Y/WvxdAt6fPZI/AAAAAAAACvE/2_bZTVZt2D8iwSeiKx1rB2rpTVbr_v9KQCLcBGAs/s1600/model3.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="592" data-original-width="1600" height="236" src="./blog_files/model3.png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Smart Compose RNN-LM model architecture. Subject and previous email message are encoded by averaging the word embeddings in each field. The averaged embeddings are then fed to the RNN-LM at each decoding step.</td></tr>
</tbody></table><b>Accelerated Model Training &amp; Serving</b><br>
Of course, once we decided on this modeling approach we still had to tune various model hyperparameters and train the models over billions of examples, all of which can be very time-intensive. To speed things up, we used a full <a href="https://cloud.google.com/tpu/">TPUv2 Pod</a> to perform experiments. In doing so, we’re able to train a model to convergence in less than a day.<br>
<br>
Even after training our faster hybrid model, our initial version of Smart Compose running on a standard CPU had an average serving latency of hundreds of milliseconds, which is still unacceptable for a feature that is trying to save users' time. Fortunately, TPUs can also be used at inference time to greatly speed up the user experience. By offloading the bulk of the computation onto TPUs, we improved the average latency to tens of milliseconds while also greatly increasing the number of requests that can be served by a single machine.<br>
<br>
<b>Fairness and Privacy</b><br>
<a href="https://g.co/mlfairness">Fairness in machine learning</a> is very important, as language understanding models can reflect human cognitive biases resulting in unwanted word associations and sentence completions. As Caliskan et al. point out in their recent paper “<a href="https://arxiv.org/abs/1608.07187">Semantics derived automatically from language corpora contain human-like biases</a>”, these associations are deeply entangled in natural language data, which presents a considerable challenge to building any language model. We are actively researching ways to continue to reduce potential biases in our training procedures. Also, since Smart Compose is trained on billions of phrases and sentences, similar to the way spam machine learning models are trained, we have done extensive testing to make sure that only common phrases used by multiple users are memorized by our model, using findings from <a href="https://arxiv.org/abs/1802.08232">this paper</a>.<br>
<br>
<b>Future work</b><br>
We are constantly working on improving the suggestion quality of the language generation model by following state-of-the-art architectures (e.g., <a href="https://arxiv.org/abs/1706.03762">Transformer,</a> <a href="https://arxiv.org/abs/1804.09849">RNMT+</a>, etc.) and experimenting with most recent and advanced training techniques. We will deploy those more advanced models to production once our strict latency constraints can be met. We are also working on incorporating personal language models, designed to more accurately emulate an individual’s style of writing into our system. <br>
<br>
<b>Acknowledgements</b><br>
<i>Smart Compose language generation model was developed by Benjamin Lee, Mia Chen, Gagan Bansal, Justin Lu, Jackie Tsay, Kaushik Roy, Tobias Bosch, Yinan Wang, Matthew Dierker, Katherine Evans, Thomas Jablin, Dehao Chen, Vinu Rajashekhar, Akshay Agrawal, Yuan Cao, Shuyuan Zhang, Xiaobing Liu, Noam Shazeer, Andrew Dai, Zhifeng Chen, Rami Al-Rfou, DK Choe, Yunhsuan Sung, Brian Strope, Timothy Sohn, Yonghui Wu, and many others.</i>
<span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Smart Compose: Using Neural Networks to Help Write Emails&amp;url=http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div>
<div class="comment-container">
<i class="comment-img material-icons">
                            
                          </i>
<span class="cmt_count_iframe_holder" data-count="0" data-onclick="" data-post-url="http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html" data-url="http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html" style="color: rgb(65, 132, 243); text-indent: 0px; margin: 0px; padding: 0px; background: transparent; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: text-top; display: inline-block; width: 76px; height: 17px;" id="_id_174060709432980560"><iframe ng-non-bindable="" frameborder="0" hspace="0" marginheight="0" marginwidth="0" scrolling="no" style="position: static; top: 0px; width: 76px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 17px;" tabindex="0" vspace="0" width="100%" id="I1_1527709895664" name="I1_1527709895664" src="./blog_files/commentcount(1).html" data-gapiattached="true" title="&lt;style&gt;body {background-color: transparent;}&lt;/style&gt;&lt;style&gt;a, span {font-family:&#39;Roboto&#39;;font-size: 14px;color: #4184f3;display: block;}&lt;/style&gt;
  &lt;script&gt;
    function reportClick() {
      var iframer = window.iframes.iframer;
      if (iframer.onclick) {
        iframer.onclick();
      }
    }
  &lt;/script&gt;
  &lt;div id=&quot;widget_bounds&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;reportClick();&quot;&gt;45 comments&lt;/a&gt;&lt;/div&gt;"></iframe></span>
</div>
<div class="post-footer">
<div class="cmt_iframe_holder" data-href="http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html" data-viewtype="FILTERED_POSTMOD"></div>
<a href="https://plus.google.com/112374322230920073195" rel="author" style="display:none;">
                        Google
                      </a>
<div class="label-footer">
<span class="labels-caption">
Labels:
</span>
<span class="labels">
<a class="label" href="http://ai.googleblog.com/search/label/AI" rel="tag">
AI
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Google%20Brain" rel="tag">
Google Brain
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Natural%20Language%20Processing" rel="tag">
Natural Language Processing
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Natural%20Language%20Understanding" rel="tag">
Natural Language Understanding
</a>
</span>
</div>
</div>
</div>
<div class="post" data-id="4050890726732103338" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html" itemprop="url" title="Automatic Photography with Google Clips">
Automatic Photography with Google Clips
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Friday, May 11, 2018
</span>
</div>
</div>
<div class="post-body">
<div class="post-content post-summary"><span class="byline-author">Posted by Aseem Agarwala, Research Scientist, Clips Content Team Lead</span><br>
 <br>
 <i>To me, photography is the simultaneous recognition, in a fraction of a second, of the significance of an event as well as of a precise organization of forms which give that event its proper expression.</i> ...<a href="http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html" itemprop="url" title="Automatic Photography with Google Clips" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Aseem Agarwala, Research Scientist, Clips Content Team Lead</span><br>
<br>
<i>To me, photography is the simultaneous recognition, in a fraction of a second, of the significance of an event as well as of a precise organization of forms which give that event its proper expression.</i><br>
— <a href="https://en.wikipedia.org/wiki/Henri_Cartier-Bresson">Henri Cartier-Bresson</a><br>
<br>
The last few years have witnessed a Cambrian-like explosion in AI, with deep learning methods enabling computer vision algorithms to recognize many of the elements of a good photograph: people, smiles, pets, sunsets, famous landmarks and more. But, despite these recent advancements, automatic photography remains a very challenging problem. Can a camera capture a great moment automatically?<br>
<br>
Recently, we released <a href="https://store.google.com/us/product/google_clips?hl=en-US">Google Clips</a>, a new, hands-free camera that automatically captures interesting moments in your life. We designed Google Clips around three important principles: <br>
<ul><li>We wanted all computations to be performed on-device. In addition to extending battery life and reducing latency, on-device processing means that none of your clips leave the device unless you decide to save or share them, which is a key privacy control.</li>
<li>We wanted the device to capture short videos, rather than single photographs. Moments with motion can be more poignant and true-to-memory, and it is often easier to shoot a video around a compelling moment than it is to capture a perfect, single instant in time.</li>
<li>We wanted  to focus on capturing candid moments of people and pets, rather than the more abstract and subjective problem of capturing artistic images. That is, we did not attempt to teach Clips to think about composition, color balance, light, etc.; instead, Clips focuses on selecting ranges of time containing people and animals doing interesting activities.</li>
</ul><b>Learning to Recognize Great Moments</b><br>
How could we train an algorithm to recognize interesting moments? As with most machine learning problems, we started with a dataset. We created a dataset of thousands of videos in diverse scenarios where we imagined Clips being used. We also made sure our dataset represented a wide range of ethnicities, genders, and ages. <a href="https://design.google/library/ux-ai/">We then hired</a> expert photographers and video editors to pore over this footage to select the best short video segments. These early curations gave us examples for our algorithms to emulate. However, it is challenging to train an algorithm solely from the subjective selection of the curators — one needs a smooth gradient of labels to teach an algorithm to recognize the quality of content, ranging from "perfect" to "terrible." <br>
<br>
To address this problem, we took a second data-collection approach, with the goal of creating a continuous quality score across the length of a video. We split each video into short segments (similar to the content Clips captures), randomly selected pairs of segments, and asked human raters to select the one they prefer.<br>
<div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-ROSz6epp32Q/WvNOMgIshgI/AAAAAAAACtA/bJTwFPpOw_IV1ncD8EGfq8TGsGtGy5wfgCLcBGAs/s1600/image3.gif" imageanchor="1" style="margin-left: 1em; margin-right: 1em;" target="_blank"><img border="0" data-original-height="414" data-original-width="800" height="330" src="./blog_files/image3(1).gif" width="640"></a></div>We took this pairwise comparison approach, instead of having raters score videos directly, because it is much easier to choose the better of a pair than it is to specify a number. We found that raters were very consistent in pairwise comparisons, and less so when scoring directly. Given enough pairwise comparisons for any given video, we were able to compute a continuous quality score over the entire length. In this process, we collected over 50,000,000 pairwise comparisons on clips sampled from over 1,000 videos. That’s a lot of human effort!<br>
<div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/dgswY1LNtI/WvNOTtBlrKI/AAAAAAAACtE/6g1d1TC4LQs7s7FULwK-6msFVGIZ17g-QCLcBGAs/s1600/image1.gif" imageanchor="1" style="margin-left: 1em; margin-right: 1em;" target="_blank"><img border="0" data-original-height="700" data-original-width="1280" height="350" src="./blog_files/image1.gif" width="640"></a></div><b>Training a Clips Quality Model</b><br>
Given this quality score training data, our next step was to train a neural network model to estimate the quality of any photograph captured by the device. We started with the basic assumption that knowing what’s <i>in</i> the photograph (e.g., people, dogs, trees, etc.) will help determine “interestingness”. If this assumption is correct, we could learn a function that uses the recognized content of the photograph to predict its quality score derived above from human comparisons. <br>
<br>
To identify content labels in our training data, we leveraged the same Google machine learning technology that powers Google image search and Google Photos, which can <a href="https://research.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html">recognize over 27,000 different labels</a> describing objects, concepts, and actions. We certainly didn’t need all these labels, nor could we compute them all on device, so our expert photographers selected the few hundred labels they felt were most relevant to predicting the “interestingness” of a photograph. We also added the labels most highly correlated with the rater-derived quality scores.<br>
<br>
Once we had this subset of labels, we then needed to design a compact, efficient model that could predict them for any given image, on-device, within strict power and thermal limits. This presented a challenge, as the deep learning techniques behind computer vision typically require strong desktop GPUs, and algorithms adapted to run on mobile devices lag far behind state-of-the-art techniques on desktop or cloud.  To train this on-device model, we first took a large set of photographs and again used Google’s powerful, server-based recognition models to predict label confidence for each of the “interesting” labels described above. We then trained a <a href="https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html">MobileNet</a> Image Content Model (ICM) to mimic the predictions of the server-based model. This compact model is capable of recognizing the most interesting elements of photographs, while ignoring non-relevant content. <br>
<br>
The final step was to predict a single quality score for an input photograph from its content predicted by the ICM, using the 50M pairwise comparisons as training data. This score is computed with a <a href="https://en.wikipedia.org/wiki/Segmented_regression">piecewise linear regression model</a> that combines the output of the ICM into a frame quality score. This frame quality score is averaged across the video segment to form a moment score. Given a pairwise comparison, our model should compute a moment score that is higher for the video segment preferred by humans. The model is trained so that its predictions match the human pairwise comparisons as well as possible.<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-OSlwx2xFUIw/WvNObrkvPDI/AAAAAAAACtI/CYY6Lw0zINQ675Gar7f50EAYq2KvTha8wCLcBGAs/s1600/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="761" data-original-width="1600" height="304" src="./blog_files/image2(1).png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Diagram of the training process for generating frame quality scores. Piecewise linear regression maps from an ICM embedding to a score which, when averaged across a video segment, yields a moment score. The moment score of the preferred segment should be higher.</td></tr>
</tbody></table>This process allowed us to train a model that combines the power of Google image recognition technology with the wisdom of human raters–represented by 50 million opinions on what makes interesting content!<br>
<br>
While this data-driven score does a great job of identifying interesting (and non-interesting) moments, we also added some bonuses to our overall quality score for phenomena that we know we want Clips to capture, including faces (especially recurring and thus “familiar” ones), smiles, and pets. In our <a href="https://blog.google/topics/hardware/jump-joy-google-clips-captures-lifes-little-moments/">most recent release</a>, we added bonuses for certain activities that customers particularly want to capture, such as hugs, kisses, jumping, and dancing. Recognizing these activities required extensions to the ICM model. <br>
<br>
<b>Shot Control</b><br>
Given this powerful model for predicting the “interestingness” of a scene, the Clips camera can decide which moments to capture in real-time. Its shot control algorithms follow three main principles:<br>
<ol><li><b>Respect Power &amp; Thermals:</b> We want the Clips battery to last roughly three hours, and we don’t want the device to overheat — the device can’t run at full throttle all the time. Clips spends much of its time in a low-power mode that captures one frame per second. If the quality of that frame exceeds a threshold set by how much Clips has recently shot, it moves into a high-power mode, capturing at 15 fps. Clips then saves a clip at the first quality peak encountered.</li>
<li><b>Avoid Redundancy:</b> We don’t want Clips to capture all of its moments at once, and ignore the rest of a session. Our algorithms therefore cluster moments into visually similar groups, and limit the number of clips in each cluster.</li>
<li><b>The Benefit of Hindsight:</b> It’s much easier to determine which clips are the best when you can examine the totality of clips captured. Clips therefore captures more moments than it intends to show to the user. When clips are ready to be transferred to the phone, the Clips device takes a second look at what it has shot, and only transfers the best and least redundant content.</li>
</ol><b>Machine Learning Fairness</b><br>
In addition to making sure our video dataset represented a diverse population, we also constructed several other tests to assess the fairness of our algorithms. We created controlled datasets by sampling subjects from different genders and skin tones in a balanced manner, while keeping variables like content type, duration, and environmental conditions constant. We then used this dataset to test that our algorithms had similar performance when applied to different groups. To help detect any regressions in fairness that might occur as we improved our moment quality models, we added fairness tests to our automated system. Any change to our software was run across this battery of tests, and was required to pass. It is important to note that this methodology can’t guarantee fairness, as we can’t test for every possible scenario and outcome. However,  we believe that these steps are an important part of our long-term work to achieve <a href="https://developers.google.com/machine-learning/fairness-overview/">fairness  in ML algorithms</a>.<br>
<br>
<b>Conclusion</b><br>
Most machine learning algorithms are designed to estimate objective qualities – a photo contains a cat, or it doesn’t. In our case, we aim to capture a more elusive and subjective quality – whether a personal photograph is interesting, or not. We therefore combine the objective, semantic content of photographs with subjective human preferences to build the AI behind Google Clips. Also, Clips is designed to work alongside a person, rather than autonomously; to get good results, a person still needs to be conscious of framing, and make sure the camera is pointed at interesting content. We’re happy with how well Google Clips performs, and are excited to continue to improve our algorithms to capture that “perfect” moment!<br>
<br>
<b>Acknowledgements</b><br>
<i>The algorithms described here were conceived and implemented by a large group of Google engineers, research scientists, and others. Figures were made by Lior Shapira. Thanks to Lior and Juston Payne for video content. </i>
<span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Automatic Photography with Google Clips&amp;url=http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div>
<div class="comment-container">
<i class="comment-img material-icons">
                            
                          </i>
<span class="cmt_count_iframe_holder" data-count="0" data-onclick="" data-post-url="http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html" data-url="http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html" style="color: rgb(65, 132, 243); text-indent: 0px; margin: 0px; padding: 0px; background: transparent; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: text-top; display: inline-block; width: 76px; height: 17px;" id="_id_240533933702526160"><iframe ng-non-bindable="" frameborder="0" hspace="0" marginheight="0" marginwidth="0" scrolling="no" style="position: static; top: 0px; width: 76px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 17px;" tabindex="0" vspace="0" width="100%" id="I2_1527709895669" name="I2_1527709895669" src="./blog_files/commentcount(2).html" data-gapiattached="true" title="&lt;style&gt;body {background-color: transparent;}&lt;/style&gt;&lt;style&gt;a, span {font-family:&#39;Roboto&#39;;font-size: 14px;color: #4184f3;display: block;}&lt;/style&gt;
  &lt;script&gt;
    function reportClick() {
      var iframer = window.iframes.iframer;
      if (iframer.onclick) {
        iframer.onclick();
      }
    }
  &lt;/script&gt;
  &lt;div id=&quot;widget_bounds&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;reportClick();&quot;&gt;14 comments&lt;/a&gt;&lt;/div&gt;"></iframe></span>
</div>
<div class="post-footer">
<div class="cmt_iframe_holder" data-href="http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html" data-viewtype="FILTERED_POSTMOD"></div>
<a href="https://plus.google.com/112374322230920073195" rel="author" style="display:none;">
                        Google
                      </a>
<div class="label-footer">
<span class="labels-caption">
Labels:
</span>
<span class="labels">
<a class="label" href="http://ai.googleblog.com/search/label/AI" rel="tag">
AI
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Computer%20Vision" rel="tag">
Computer Vision
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Hardware" rel="tag">
Hardware
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/video" rel="tag">
video
</a>
</span>
</div>
</div>
</div>
<div class="post" data-id="647197827577198347" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html" itemprop="url" title="Custom On-Device ML Models with Learn2Compress">
Custom On-Device ML Models with Learn2Compress
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Wednesday, May 9, 2018
</span>
</div>
</div>
<div class="post-body">
<div class="post-content post-summary"><span class="byline-author">Posted by Sujith Ravi, Senior Staff Research Scientist, Google Expander Team</span>   <br>
 <br>
Successful deep learning models often require significant amounts  of computational resources, memory and power to train and run, which presents an obstacle if you want them to perform well on mobile and IoT devices.<a href="http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html" itemprop="url" title="Custom On-Device ML Models with Learn2Compress" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Sujith Ravi, Senior Staff Research Scientist, Google Expander Team</span>  <br>
<br>
Successful deep learning models often require significant amounts  of computational resources, memory and power to train and run, which presents an obstacle if you want them to perform well on mobile and IoT devices. <a href="https://research.googleblog.com/2017/02/on-device-machine-intelligence.html">On-device machine learning</a> allows you to run inference directly on the devices, with the benefits of data privacy and access everywhere, regardless of connectivity. On-device ML systems, such as <a href="https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html">MobileNets</a> and <a href="https://research.googleblog.com/2017/11/on-device-conversational-modeling-with.html">ProjectionNets</a>, address the resource bottlenecks on mobile devices by optimizing for model efficiency. But what if you wanted to train your <i>own</i> customized, on-device models for your personal mobile application? <br>
<br>
Yesterday at Google I/O, we announced <a href="https://developers.google.com/ml-kit/">ML Kit</a> to make machine learning accessible for all mobile developers. One of the core ML Kit capabilities that will be available soon is an automatic model compression service powered by “Learn2Compress” technology developed by our research team. Learn2Compress enables custom on-device deep learning models in <a href="https://developers.googleblog.com/2017/11/announcing-tensorflow-lite.html">TensorFlow Lite</a> that run efficiently on mobile devices, without developers having to worry about optimizing for memory and speed. We are pleased to make Learn2Compress for image classification available soon through ML Kit. Learn2Compress will be initially available to a small number of developers, and will be offered more broadly in the coming months. You can sign up <a href="https://g.co/firebase/signup">here</a> if you are interested in using this feature for building your own models.<br>
<br>
<b>How it Works</b><br>
Learn2Compress generalizes the learning framework introduced in previous works like <a href="https://arxiv.org/abs/1708.00630">ProjectionNet</a> and incorporates several state-of-the-art techniques for compressing neural network models. It takes as input a large pre-trained TensorFlow model provided by the user, performs training and optimization and automatically generates ready-to-use on-device models that are smaller in size, more memory-efficient, more power-efficient and faster at inference with minimal loss in accuracy.<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-rLAjT1bpCKk/WvN5OhmkkcI/AAAAAAAACuA/-N1hoYLhDdcjk1qZy279lontG7dshf9QgCLcBGAs/s1600/f1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="521" data-original-width="1600" height="208" src="./blog_files/f1.png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Learn2Compress for automatically generating on-device ML models.</td></tr>
</tbody></table>
To do this,  Learn2Compress uses multiple neural network optimization and compression techniques including:<br>
<ul>
<li><i>Pruning</i> reduces model size by removing weights or operations that are least useful for predictions (e.g.low-scoring weights). This can be very effective especially for <a href="https://research.googleblog.com/2017/11/on-device-conversational-modeling-with.html">on-device models</a> involving sparse inputs or outputs, which can be reduced up to 2x in size while retaining 97% of the original prediction quality.<br>
</li>
<li><a href="https://arxiv.org/abs/1712.05877"><i>Quantization</i></a> techniques are particularly effective when applied during training and can improve inference speed by reducing the number of bits used for model weights and activations. For example, using 8-bit fixed point representation instead of floats can speed up the model inference, reduce power and further reduce size by 4x.</li>
<li><i><a href="https://arxiv.org/abs/1708.00630">Joint training</a></i> and <a href="https://arxiv.org/abs/1503.02531"><i>distillation</i></a> approaches follow a teacher-student learning strategy — we use a larger teacher network (in this case, user-provided TensorFlow model) to train a compact student network (on-device model) with minimal loss in accuracy.<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://3.bp.blogspot.com/-eD3Mc4FLsvA/WvN6BweGY_I/AAAAAAAACuU/OZqGR1UUvL05Ctr1b8JD3SaKlCNCZVMdACLcBGAs/s1600/f2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="1044" data-original-width="1502" height="444" src="./blog_files/f2.png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Joint training and distillation approach to learn compact student models.</td></tr>
</tbody></table>
The teacher network can be fixed (as in distillation) or jointly optimized, and even train multiple student models of different sizes simultaneously. So instead of a single model, Learn2Compress generates multiple on-device models in a single shot, at different sizes and inference speeds, and lets the developer pick one best suited for their application needs.<br>
</li>
</ul>
These and other techniques like <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> also make the compression process more efficient and scalable to large-scale datasets.<br>
<br>
<b>How well does it work?</b><br>
To demonstrate the effectiveness of Learn2Compress, we used it to build compact on-device models of several state-of-the-art deep networks used in image and natural language tasks such as <a href="https://arxiv.org/abs/1704.04861">MobileNets</a>, <a href="https://arxiv.org/abs/1707.07012">NASNet</a>, <a href="https://arxiv.org/abs/1512.00567">Inception</a>, <a href="https://arxiv.org/abs/1708.00630">ProjectionNet</a>, among others. For a given task and dataset, we can generate multiple on-device models at different inference speeds and model sizes. <br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://4.bp.blogspot.com/-vhIB65lfBbo/WvN5iGy2HkI/AAAAAAAACuI/0fT8SIYfZaEGG3CyLPbE3jVK7BGNMjD1wCLcBGAs/s1600/f3.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="620" data-original-width="1600" height="248" src="./blog_files/f3.png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Accuracy at various sizes for Learn2Compress models and full-sized baseline networks on <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> (left) and <a href="http://image-net.org/">ImageNet</a> (right) image classification tasks. Student networks used to produce the compressed variants for CIFAR-10 and ImageNet are modeled using <a href="https://arxiv.org/abs/1707.07012">NASNet</a> and <a href="https://arxiv.org/abs/1704.04861">MobileNet</a>-inspired architectures, respectively.</td></tr>
</tbody></table>
For image classification, Learn2Compress can generate small and fast models with good prediction accuracy suited for mobile applications. For example, on <a href="http://image-net.org/">ImageNet</a> task, Learn2Compress achieves a model 22x smaller than Inception v3 baseline and 4x smaller than MobileNet v1 baseline with just 4.6-7% drop in accuracy. On <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>, jointly training multiple Learn2Compress models with shared parameters, takes only 10% more time than training a single Learn2Compress large model, but yields 3 compressed models that are upto 94x smaller in size and upto 27x faster with up to 36x lower cost and good prediction quality (90-95% top-1 accuracy).<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://3.bp.blogspot.com/-nrWYQszTHrA/WvN5yN02aCI/AAAAAAAACuQ/x8FpayO0_kIFJwLGg5EaQR4_qpMD5JK4QCLcBGAs/s1600/f4.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="656" data-original-width="1600" height="262" src="./blog_files/f4.png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Computation cost and average prediction latency (on Pixel phone) for baseline and Learn2Compress models on <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> image classification task. Learn2Compress-optimized models use <a href="https://arxiv.org/abs/1707.07012">NASNet</a>-style network architecture.</td></tr>
</tbody></table>
We are also excited to see how well this performs on developer use-cases. For example, <a href="https://fishbrain.com/">Fishbrain</a>, a social platform for fishing enthusiasts, used Learn2Compress to compress their existing image classification cloud model (80MB+ in size and 91.8% top-3 accuracy) to a much smaller on-device model, less than 5MB in size, with similar accuracy. In some cases, we observe that it is possible for the compressed models to even slightly outperform the original large model’s accuracy due to better regularization effects.<br>
<br>
We will continue to improve Learn2Compress with future advances in ML and deep learning, and extend to more use-cases beyond image classification. We are excited and looking forward to make this available soon through ML Kit’s compression service on the Cloud. We hope this will make it easy for developers to automatically build and optimize their own on-device ML models so that they can focus on building great apps and cool user experiences involving computer vision, natural language and other machine learning applications.<br>
<br>
<b>Acknowledgments</b><br>
<i>I would like to acknowledge our core contributors Gaurav Menghani, Prabhu Kaliamoorthi and Yicheng Fan along with Wei Chai, Kang Lee, Sheng Xu and Pannag Sanketi. Special thanks to Dave Burke, Brahim Elbouchikhi, Hrishikesh Aradhye, Hugues Vincent, and Arun Venkatesan from the Android team; Sachin Kotwani, Wesley Tarle, Pavel Jbanov and from the Firebase team; Andrei Broder, Andrew Tomkins, Robin Dua, Patrick McGregor, Gaurav Nemade, the Google Expander team and TensorFlow team.</i><br>
<br>
<br>
<span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Custom On-Device ML Models with Learn2Compress&amp;url=http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div>
<div class="comment-container">
<i class="comment-img material-icons">
                            
                          </i>
<span class="cmt_count_iframe_holder" data-count="0" data-onclick="" data-post-url="http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html" data-url="http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html" style="color: rgb(65, 132, 243); text-indent: 0px; margin: 0px; padding: 0px; background: transparent; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: text-top; display: inline-block; width: 69px; height: 17px;" id="_id_319701572447803150"><iframe ng-non-bindable="" frameborder="0" hspace="0" marginheight="0" marginwidth="0" scrolling="no" style="position: static; top: 0px; width: 69px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 17px;" tabindex="0" vspace="0" width="100%" id="I3_1527709895676" name="I3_1527709895676" src="./blog_files/commentcount(3).html" data-gapiattached="true" title="&lt;style&gt;body {background-color: transparent;}&lt;/style&gt;&lt;style&gt;a, span {font-family:&#39;Roboto&#39;;font-size: 14px;color: #4184f3;display: block;}&lt;/style&gt;
  &lt;script&gt;
    function reportClick() {
      var iframer = window.iframes.iframer;
      if (iframer.onclick) {
        iframer.onclick();
      }
    }
  &lt;/script&gt;
  &lt;div id=&quot;widget_bounds&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;reportClick();&quot;&gt;8 comments&lt;/a&gt;&lt;/div&gt;"></iframe></span>
</div>
<div class="post-footer">
<div class="cmt_iframe_holder" data-href="http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html" data-viewtype="FILTERED_POSTMOD"></div>
<a href="https://plus.google.com/112374322230920073195" rel="author" style="display:none;">
                        Google
                      </a>
<div class="label-footer">
<span class="labels-caption">
Labels:
</span>
<span class="labels">
<a class="label" href="http://ai.googleblog.com/search/label/Computer%20Vision" rel="tag">
Computer Vision
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Deep%20Learning" rel="tag">
Deep Learning
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Expander" rel="tag">
Expander
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Machine%20Learning" rel="tag">
Machine Learning
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/On-device%20Learning" rel="tag">
On-device Learning
</a>
</span>
</div>
</div>
</div>
<div class="post" data-id="3989663299929022789" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html" itemprop="url" title="Google Duplex: An AI System for Accomplishing Real-World Tasks Over the Phone">
Google Duplex: An AI System for Accomplishing Real-World Tasks Over the Phone
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Tuesday, May 8, 2018
</span>
</div>
</div>
<div class="post-body">
<div class="post-content post-summary"><span class="byline-author">Posted by Yaniv Leviathan, Principal Engineer and Yossi Matias, Vice President, Engineering, Google</span>   <br>
 <br>
A long-standing goal of human-computer interaction has been to enable people to have a natural conversation with computers, as they would with each other. In recent years, we have witnessed a revolution in the ability of computers to understand and to generate natural speech, especially with the application of deep neural networks (e.g. ...<a href="http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html" itemprop="url" title="Google Duplex: An AI System for Accomplishing Real-World Tasks Over the Phone" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Yaniv Leviathan, Principal Engineer and Yossi Matias, Vice President, Engineering, Google</span>  <br>
<br>
A long-standing goal of human-computer interaction has been to enable people to have a natural conversation with computers, as they would with each other. In recent years, we have witnessed a revolution in the ability of computers to understand and to generate natural speech, especially with the application of deep neural networks (e.g., <a href="https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html">Google voice search</a>, <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">WaveNet</a>). Still, even with today’s state of the art systems, it is often frustrating having to talk to stilted computerized voices that don't understand natural language. In particular, automated phone systems are still struggling to recognize simple words and commands. They don’t engage in a conversation flow and force the caller to adjust to the system instead of the system adjusting to the caller.<br>
<br>
Today we announce Google Duplex, a new technology for conducting natural conversations to carry out “real world” tasks over the phone. The technology is directed towards completing specific tasks, such as scheduling certain types of appointments. For such tasks, the system makes the conversational experience as natural as possible, allowing people to speak normally, like they would to another person, without having to adapt to a machine.<br>
<br>
One of the key research insights was to constrain Duplex to closed domains, which are narrow enough to explore extensively. Duplex can only carry out natural conversations after being deeply trained in such domains. It cannot carry out general conversations.<br>
<br>
Here are examples of Duplex making phone calls (using different voices):<br>
<table style="margin-left: auto; margin-right: auto; text-align: right;"><tbody>
<tr><td>Duplex scheduling a hair salon appointment: </td>   <td><audio controls="controls" src="http://www.gstatic.com/b-g/DMS03IIQXU3TY2FD6DLPLOMBBBJ2CH188143148.mp3"></audio></td>   </tr>
<tr><td>Duplex calling a restaurant: </td>     <td><audio controls="controls" src="http://www.gstatic.com/b-g/KOK4HAMTAPH5Z96154F6GKUM74A3Z1576269077.mp3"></audio></td>   </tr>
</tbody></table><br>
While sounding natural, these and other examples are conversations between a fully automatic computer system and real businesses.<br>
<br>
The Google Duplex technology is built to sound natural, to make the conversation experience comfortable. It’s important to us that users and businesses have a good experience with this service, and transparency is a key part of that. We want to be clear about the intent of the call so businesses understand the context. We’ll be experimenting with the right approach over the coming months. <br>
<br>
<b>Conducting Natural Conversations</b><br>
There are several challenges in conducting natural conversations: natural language is hard to understand, natural behavior is tricky to model, latency expectations require fast processing, and generating natural sounding speech, with the appropriate intonations, is difficult.<br>
<br>
When people talk to each other, they use more complex sentences than when talking to computers. They often correct themselves mid-sentence, are more verbose than necessary, or omit words and rely on context instead; they also express a wide range of intents, sometimes in the same sentence, e.g., <i>“So umm Tuesday through Thursday we are open 11 to 2, and then reopen 4 to 9, and then Friday, Saturday, Sunday we... or Friday, Saturday we're open 11 to 9 and then Sunday we're open 1 to 9.”</i><br>
<table style="margin-left: auto; margin-right: auto; text-align: right;"><tbody>
<tr><td>Example of complex statement: </td>   <td><audio controls="controls" src="http://www.gstatic.com/b-g/BT5EH08P73O41Q94PTWNMV42DAWU8Z192313240.mp3"></audio></td></tr>
</tbody></table><br>
In natural spontaneous speech people talk faster and less clearly than they do when they speak to a machine, so speech recognition is harder and we see higher word error rates. The problem is aggravated during phone calls, which often have loud background noises and sound quality issues.<br>
<br>
In longer conversations, the same sentence can have very different meanings depending on context. For example, when booking reservations “Ok for 4” can mean the time of the reservation or the number of people. Often the relevant context might be several sentences back, a problem that gets compounded by the increased word error rate in phone calls.<br>
<div class="separator" style="clear: both; text-align: center;"><a href="https://3.bp.blogspot.com/-JHI_4p_unYc/WvEYZpKshbI/AAAAAAAACr8/xzh0Rza1XuYlPIhoI6Bo-umzIN4h4DVWQCLcBGAs/s1600/ok_for_4_big.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;" target="_blank"><img border="0" data-original-height="383" data-original-width="1600" height="153" src="./blog_files/ok_for_4_big.png" width="640"></a></div><br>
Deciding what to say is a function of both the task and the state of the conversation. In addition, there are some common practices in natural conversations — implicit protocols that include <i>elaborations </i>(“for next Friday” “for when?” “for Friday next week, the 18th.”), <i>syncs</i> (“can you hear me?”), <i>interruptions</i> (“the number is 212-” “sorry can you start over?”), and <i>pauses</i> (“can you hold? [pause] thank you!” different meaning for a pause of 1 second vs 2 minutes).<br>
<br>
<b>Enter Duplex</b><br>
Google Duplex’s conversations sound natural thanks to advances in <i>understanding</i>, <i>interacting</i>, <i>timing</i>, and <i>speaking</i>.<br>
<br>
At the core of Duplex is a <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network</a> (RNN) designed to cope with these challenges, built using <a href="https://www.tensorflow.org/tfx">TensorFlow Extended</a> (TFX). To obtain its high precision, we trained Duplex’s RNN on a corpus of anonymized phone conversation data. The network uses the output of Google’s automatic speech recognition (ASR) technology, as well as features from the audio, the history of the conversation, the parameters of the conversation (e.g. the desired service for an appointment, or the current time of day) and more. We trained our understanding model separately for each task, but leveraged the shared corpus across tasks. Finally, we used hyperparameter optimization from TFX to further improve the model.<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-WdgRuOg6lwc/WvEZTDLYg_I/AAAAAAAACsM/QNLSl4Yid9wKij_2KNpCRiXjiyeptu9vgCLcBGAs/s1600/rnn_big.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="638" data-original-width="1600" height="255" src="./blog_files/rnn_big.png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Incoming sound is processed through an ASR system. This produces text that is analyzed with context data and other inputs to produce a response text that is read aloud through the TTS system.</td></tr>
</tbody></table><table style="margin-left: auto; margin-right: auto; text-align: right;"><tbody>
<tr><td>Duplex handling interruptions: </td>   <td><audio controls="controls" src="http://www.gstatic.com/b-g/OROY9DN8QUHYUN1VED9V1QS0387EOX207713725.mp3"></audio></td>   </tr>
<tr><td>Duplex elaborating: </td>   <td><audio controls="controls" src="http://www.gstatic.com/b-g/YBFPW2YQBZPVP4WONSIUO24KV82NY32653447.mp3"></audio></td>   </tr>
<tr><td>Duplex responding to a sync: </td>     <td><audio controls="controls" src="http://www.gstatic.com/b-g/ZIC83HEVAJKTZ3SBUQFEWSXTH0O7916575831.mp3"></audio></td>   </tr>
</tbody></table><br>
<b>Sounding Natural</b><br>
We use a combination of a concatenative text to speech (TTS) engine and a synthesis TTS engine (using <a href="https://research.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html">Tacotron</a> and <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">WaveNet</a>) to control intonation depending on the circumstance. <br>
<br>
The system also sounds more natural thanks to the incorporation of speech disfluencies (e.g. “hmm”s and “uh”s). These are added when combining widely differing sound units in the concatenative TTS or adding synthetic waits, which allows the system to signal in a natural way that it is still processing. (This is what people often do when they are gathering their thoughts.) In user studies, we found that conversations using these disfluencies sound more familiar and natural.<br>
<br>
Also, it’s important for <i>latency</i> to match people’s expectations. For example, after people say something simple, e.g., “hello?”, they expect an instant response, and are more sensitive to latency. When we detect that low latency is required, we use faster, low-confidence models (e.g. speech recognition or endpointing). In extreme cases, we don’t even wait for our RNN, and instead use faster approximations (usually coupled with more hesitant responses, as a person would do if they didn’t fully understand their counterpart). This allows us to have less than 100ms of response latency in these situations. Interestingly, in some situations, we found it was actually helpful to introduce <i>more</i> latency to make the conversation feel more natural — for example, when replying to a really complex sentence.<br>
<br>
<b>System Operation</b><br>
The Google Duplex system is capable of carrying out sophisticated conversations and it completes the majority of its tasks <i>fully autonomously</i>, without human involvement. The system has a self-monitoring capability, which allows it to recognize the tasks it cannot complete autonomously (e.g., scheduling an unusually complex appointment). In these cases, it signals to a human operator, who can complete the task.<br>
<br>
To train the system in a new domain, we use <i>real-time supervised training</i>. This is comparable to the training practices of many disciplines, where an instructor supervises a student as they are doing their job, providing guidance as needed, and making sure that the task is performed at the instructor’s level of quality. In the Duplex system, experienced operators act as the instructors. By monitoring the system as it makes phone calls in a new domain, they can affect the behavior of the system in real time as needed. This continues until the system performs at the desired quality level, at which point the supervision stops and the system can make calls autonomously.<br>
<br>
<b>Benefits for Businesses and Users</b><br>
Businesses that rely on appointment bookings supported by Duplex, and are not yet powered by online systems, can benefit from Duplex by allowing customers to book through the Google Assistant without having to change any day-to-day practices or train employees. Using Duplex could also reduce no-shows to appointments by reminding customers about their upcoming appointments in a way that allows easy cancellation or rescheduling.<br>
<table style="margin-left: auto; margin-right: auto; text-align: right;"><tbody>
<tr><td>Duplex calling a restaurant: </td>      <td><audio controls="controls" src="http://www.gstatic.com/b-g/5717BWTLRKCBB8JUDQ1QUA6HMC26WL238301173.mp3"></audio></td>   </tr>
</tbody></table><br>
In another example, customers often call businesses to inquire about information that is not available online such as hours of operation during a holiday. Duplex can call the business to inquire about open hours and make the information available online with Google, reducing the number of such calls businesses receive, while at the same time, making the information more accessible to everyone. Businesses can operate as they always have, there’s no learning curve or changes to make to benefit from this technology.<br>
<table style="margin-left: auto; margin-right: auto; text-align: right;"><tbody>
<tr><td>Duplex asking for holiday hours: </td>   <td><audio controls="controls" src="http://www.gstatic.com/b-g/N6WGB2KL3NZCVUJAGOX3BLZRJ31BRP278409629.mp3"></audio></td>   </tr>
</tbody></table><br>
For users, Google Duplex is making supported tasks easier. Instead of making a phone call, the user simply interacts with the Google Assistant, and the call happens completely in the background without any user involvement.<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-M75VzGyBSkM/WvEYy1jYSOI/AAAAAAAACsE/xAPLvXZe9cI7_YXkh8s1WF0bUHw9aNKRQCLcBGAs/s1600/end_to_end_big.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="326" data-original-width="1600" height="130" src="./blog_files/end_to_end_big.png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">A user asks the Google Assistant for an appointment, which the Assistant then schedules by having Duplex call the business.</td></tr>
</tbody></table>Another benefit for users is that Duplex enables delegated communication with service providers in an asynchronous way, e.g., requesting reservations during off-hours, or with limited connectivity. It can also help address accessibility and language barriers, e.g., allowing hearing-impaired users, or users who don’t speak the local language, to carry out tasks over the phone. <br>
<br>
This summer, we’ll start testing the Duplex technology within the <a href="http://blog.google/products/assistant/io18">Google Assistant</a>, to help users make restaurant reservations, schedule hair salon appointments, and get holiday hours over the phone.<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://3.bp.blogspot.com/-Arp_jhtS4F0/WvD7KzLvDRI/AAAAAAAACrs/CgWeSM1I3okK5uyCt0d9BsYhtKBnpKuRACLcBGAs/s1600/Yaniv%2BMatan%2BFinal.jpeg" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="1593" data-original-width="1600" height="398" src="./blog_files/Yaniv Matan Final.jpeg" width="400"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Yaniv Leviathan, Google Duplex lead, and Matan Kalman, engineering manager on the project, enjoying a meal booked through a call from Duplex.</td></tr>
</tbody></table><table style="margin-left: auto; margin-right: auto; text-align: right;"><tbody>
<tr><td>Duplex calling to book the above meal: </td>   <td><audio controls="controls" src="http://www.gstatic.com/b-g/1RYL7HFNYMJUOLXWHMB4TOYIDA7YFR191723752.mp3"></audio></td>   </tr>
</tbody></table><br>
Allowing people to interact with technology as naturally as they interact with each other has been a long standing promise. Google Duplex takes a step in this direction, making interaction with technology via natural conversation a reality in specific scenarios. We hope that these technology advances will ultimately contribute to a meaningful improvement in people’s experience in day-to-day interactions with computers.
<span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Google Duplex: An AI System for Accomplishing Real-World Tasks Over the Phone&amp;url=http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div>
<div class="comment-container">
<i class="comment-img material-icons">
                            
                          </i>
<span class="cmt_count_iframe_holder" data-count="0" data-onclick="" data-post-url="http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html" data-url="http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html" style="color: rgb(65, 132, 243); text-indent: 0px; margin: 0px; padding: 0px; background: transparent; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: text-top; display: inline-block; width: 83px; height: 17px;" id="_id_480145311142539540"><iframe ng-non-bindable="" frameborder="0" hspace="0" marginheight="0" marginwidth="0" scrolling="no" style="position: static; top: 0px; width: 83px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 17px;" tabindex="0" vspace="0" width="100%" id="I4_1527709895683" name="I4_1527709895683" src="./blog_files/commentcount(4).html" data-gapiattached="true" title="&lt;style&gt;body {background-color: transparent;}&lt;/style&gt;&lt;style&gt;a, span {font-family:&#39;Roboto&#39;;font-size: 14px;color: #4184f3;display: block;}&lt;/style&gt;
  &lt;script&gt;
    function reportClick() {
      var iframer = window.iframes.iframer;
      if (iframer.onclick) {
        iframer.onclick();
      }
    }
  &lt;/script&gt;
  &lt;div id=&quot;widget_bounds&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;reportClick();&quot;&gt;391 comments&lt;/a&gt;&lt;/div&gt;"></iframe></span>
</div>
<div class="post-footer">
<div class="cmt_iframe_holder" data-href="http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html" data-viewtype="FILTERED_POSTMOD"></div>
<a href="https://plus.google.com/112374322230920073195" rel="author" style="display:none;">
                        Google
                      </a>
<div class="label-footer">
<span class="labels-caption">
Labels:
</span>
<span class="labels">
<a class="label" href="http://ai.googleblog.com/search/label/AI" rel="tag">
AI
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/HCI" rel="tag">
HCI
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Machine%20Learning" rel="tag">
Machine Learning
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Natural%20Language%20Understanding" rel="tag">
Natural Language Understanding
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Speech%20Recognition" rel="tag">
Speech Recognition
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/TTS" rel="tag">
TTS
</a>
</span>
</div>
</div>
</div>
<div class="post" data-id="5360038134097375871" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html" itemprop="url" title="Deep Learning for Electronic Health Records">
Deep Learning for Electronic Health Records
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Tuesday, May 8, 2018
</span>
</div>
</div>
<div class="post-body">
<div class="post-content post-summary"><span class="byline-author">Posted by Alvin Rajkomar MD, Research Scientist and Eyal Oren PhD, Product Manager, Google AI</span><br>
 <br>
When patients get admitted to a hospital, they have many questions about what will happen next.  When will I be able to go home? Will I get better?  Will I have to come back to the hospital? Having precise answers to those questions helps doctors and nurses make care better, safer, and faster — if a patient’s health is deteriorating, doctors could be sent proactively to act before things get worse.<a href="http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html" itemprop="url" title="Deep Learning for Electronic Health Records" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Alvin Rajkomar MD, Research Scientist and Eyal Oren PhD, Product Manager, Google AI</span><br>
<br>
When patients get admitted to a hospital, they have many questions about what will happen next.  When will I be able to go home? Will I get better?  Will I have to come back to the hospital? Having precise answers to those questions helps doctors and nurses make care better, safer, and faster — if a patient’s health is deteriorating, doctors could be sent proactively to act before things get worse.<br>
<br>
Predicting what will happen next is a natural application of machine learning.  We wondered if the same types of machine learning that predict traffic during your commute or the next word in a translation from English to Spanish could be used for clinical predictions. For predictions to be useful in practice they should be, at least:<br>
<ol><li><b>Scalable:</b>  Predictions should be straightforward to create for any important outcome and for different hospital systems. Since healthcare data is very complicated and requires much <a href="https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html">data wrangling</a>, this requirement is not straightforward to satisfy.</li>
<li><b>Accurate:</b>  Predictions should alert clinicians to problems but not distract them with false alarms. With the widespread adoption of electronic health records, we set out to use that data to create more accurate prediction models.</li>
</ol>Together with colleagues at <a href="http://ichs.ucsf.edu/">UC San Francisco</a>, <a href="http://med.stanford.edu/">Stanford Medicine</a>, and <a href="https://hdsi.uchicago.edu/">The University of Chicago Medicine</a>, we published “<a href="https://www.nature.com/articles/s41746-018-0029-1">Scalable and Accurate Deep Learning with Electronic Health Records</a>” in <a href="https://www.nature.com/npjdigitalmed/">Nature Partner Journals: Digital Medicine</a>, which contributes to these two aims.<br>
<div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-L0AW4pdJBu8/WvGyhIDyG9I/AAAAAAAACsc/r_vm37nIjg0xiXM5Q7oLQ-mhq9ncAPbYACLcBGAs/s1600/image2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;" target="_blank"><img border="0" data-original-height="657" data-original-width="1600" height="262" src="./blog_files/image2(2).png" width="640"></a></div>We used deep learning models to make a broad set of predictions relevant to hospitalized patients using de-identified electronic health records. Importantly, we were able to use the data as-is, without the laborious manual effort typically required to extract, clean, harmonize, and transform relevant variables in those records.  Our partners had removed sensitive individual information before we received it, and on our side, we protected the data using state-of-the-art security including logical separation, strict access controls, and encryption of data at rest and in transit.<br>
<br>
<b>Scalability</b><br>
Electronic health records (EHRs) are tremendously complicated.  Even a temperature measurement has a different meaning depending on if it’s taken under the tongue, through your eardrum, or on your forehead.  And that's just a simple vital sign. Moreover, each health system customizes their EHR system, making the data collected at one hospital look different than data on a similar patient receiving similar care at another hospital.  Before we could even apply machine learning, we needed a consistent way to represent patient records, which we built on top of the open <a href="https://en.wikipedia.org/wiki/Fast_Healthcare_Interoperability_Resources">Fast Healthcare Interoperability Resources</a> (FHIR) standard as described <a href="https://ai.googleblog.com/2018/03/making-healthcare-data-work-better-with.html">in an earlier blog post</a>.<br>
<br>
Once in a consistent format, we did not have to manually select or harmonize the variables to use. Instead, for each prediction, a deep learning model reads all the data-points from earliest to most recent and then learns which data helps predict the outcome. Since there are thousands of data points involved, we had to develop some new types of deep learning modeling approaches based on <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural networks</a> (RNNs) and <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">feedforward</a> networks. <br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-lz949-xQzIo/WvGzL3qUXfI/AAAAAAAACsk/I_p1OgsWw70uRPlVN5kzErIpcclktYLhACLcBGAs/s1600/image3.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="851" data-original-width="1600" height="340" src="./blog_files/image3.png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Data in a patient's record is represented as a timeline.  For illustrative purposes, we display various types of clinical data (e.g. encounters, lab tests) by row.  Each piece of data, indicated as a little grey dot, is stored in <a href="https://en.wikipedia.org/wiki/Fast_Healthcare_Interoperability_Resources">FHIR</a>, an open data standard that can be used by any healthcare institution. A deep learning model analyzed a patient's chart by reading the timeline from left to right, from the beginning of a chart to the current hospitalization, and used this data to make different types of predictions.</td></tr>
</tbody></table>Thus we engineered a computer system to render predictions without hand-crafting a new dataset for each task, in a scalable manner. But setting up the data is only one part of the work; the predictions also need to be accurate. <br>
<br>
<b>Prediction Accuracy</b><br>
The most common way to assess accuracy is by a measure called the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">area-under-the-receiver-operator curve</a>, which measures how well a model distinguishes between a patient who will have a particular future outcome compared to one who will not. In this metric, 1.00 is perfect, and 0.50 is no better than random chance, so higher numbers mean the model is more accurate. By this measure, the models we reported in the paper scored 0.86 in predicting if patients will stay long in the hospital (traditional logistic regression scored 0.76); they scored 0.95 in predicting inpatient mortality (traditional methods were 0.86), and they scored 0.77 in predicting unexpected readmissions after patients are discharged (traditional methods were 0.70). These gains were statistically significant.<br>
<br>
We also used these models to identify the conditions for which the patients were being treated. For example, if a doctor prescribed ceftriaxone and doxycycline for a patient with an elevated temperature, fever and cough, the model could identify these as signals that the patient was being treated for pneumonia. We emphasize that the model is <i>not</i> diagnosing patients — it picks up signals about the patient, their treatments and notes written by their clinicians, so the model is more like a good listener than a master diagnostician.   <br>
<br>
An important focus of our work includes the interpretability of the deep learning models used. An “attention map” of each prediction shows the important data points considered by the models as they make that prediction.  We show an example as a proof-of-concept and see this as an important part of what makes predictions useful for clinicians.  <br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-fgxR15HQsWU/WvG0ZQPsbSI/AAAAAAAACsw/nGi4TlHCq3wfuqVePRY9j9n97VQIKCtBQCLcBGAs/s1600/image1.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="1149" data-original-width="1600" height="458" src="./blog_files/image1.jpg" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">A deep learning model was used to render a prediction 24 hours after a patient was admitted to the hospital.  The timeline (top of figure) contains months of historical data and the most recent data is shown enlarged in the middle.  The model "attended" to information highlighted in red that was in the patient's chart to "explain" its prediction.  In this case-study, the model highlighted pieces of information that make sense clinically. Figure from our <a href="https://www.nature.com/articles/s41746-018-0029-1">paper</a>.</td></tr>
</tbody></table><b>What does this mean for patients and clinicians?</b><br>
The results of this work are early and on retrospective data only.  Indeed, this paper represents just the beginning of the work that is needed to test the hypothesis that machine learning can be used to make healthcare better.  Doctors are already inundated with alerts and demands on their attention — could models help physicians with tedious, administrative tasks so they can better focus on the patient in front of them or ones that need extra attention?  Can we help patients get high-quality care no matter where they seek it? We look forward to collaborating with doctors and patients to figure out the answers to these questions and more. 
<span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Deep Learning for Electronic Health Records&amp;url=http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div>
<div class="comment-container">
<i class="comment-img material-icons">
                            
                          </i>
<span class="cmt_count_iframe_holder" data-count="0" data-onclick="" data-post-url="http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html" data-url="http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html" style="color: rgb(65, 132, 243); text-indent: 0px; margin: 0px; padding: 0px; background: transparent; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: text-top; display: inline-block; width: 76px; height: 17px;" id="_id_582084365590862320"><iframe ng-non-bindable="" frameborder="0" hspace="0" marginheight="0" marginwidth="0" scrolling="no" style="position: static; top: 0px; width: 76px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 17px;" tabindex="0" vspace="0" width="100%" id="I5_1527709895700" name="I5_1527709895700" src="./blog_files/commentcount(5).html" data-gapiattached="true" title="&lt;style&gt;body {background-color: transparent;}&lt;/style&gt;&lt;style&gt;a, span {font-family:&#39;Roboto&#39;;font-size: 14px;color: #4184f3;display: block;}&lt;/style&gt;
  &lt;script&gt;
    function reportClick() {
      var iframer = window.iframes.iframer;
      if (iframer.onclick) {
        iframer.onclick();
      }
    }
  &lt;/script&gt;
  &lt;div id=&quot;widget_bounds&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;reportClick();&quot;&gt;29 comments&lt;/a&gt;&lt;/div&gt;"></iframe></span>
</div>
<div class="post-footer">
<div class="cmt_iframe_holder" data-href="http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html" data-viewtype="FILTERED_POSTMOD"></div>
<a href="https://plus.google.com/112374322230920073195" rel="author" style="display:none;">
                        Google
                      </a>
<div class="label-footer">
<span class="labels-caption">
Labels:
</span>
<span class="labels">
<a class="label" href="http://ai.googleblog.com/search/label/Deep%20Learning" rel="tag">
Deep Learning
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Google%20Brain" rel="tag">
Google Brain
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Health" rel="tag">
Health
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Machine%20Learning" rel="tag">
Machine Learning
</a>
</span>
</div>
</div>
</div>
<div class="post" data-id="9018433170406683087" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/05/introducing-google-ai.html" itemprop="url" title="Introducing Google AI">
Introducing Google AI
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Monday, May 7, 2018
</span>
</div>
</div>
<div class="post-body">
<div class="post-content post-summary"><span class="byline-author">Posted by Christian Howard, Editor-in-Chief, Google AI Communications</span><br>
 <br>
 <i>Hmm, have I made a wrong turn?  I was looking for Google Research…</i><br>
 <br>
For the past several years, we’ve pursued research that reflects our commitment to make AI available for everyone. From ...<a href="http://ai.googleblog.com/2018/05/introducing-google-ai.html" itemprop="url" title="Introducing Google AI" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Christian Howard, Editor-in-Chief, Google AI Communications</span><br>
<br>
<i>Hmm, have I made a wrong turn?  I was looking for Google Research…</i><br>
<br>
For the past several years, we’ve pursued research that reflects our commitment to make AI available for everyone. From <a href="https://research.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html">computer vision</a> to <a href="https://research.googleblog.com/search/label/Health">healthcare research</a> to <a href="https://research.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html">AutoML</a>, we have increasingly put emphasis on implementing machine learning techniques in nearly everything we do at Google. Our research has been core to the development and integration of these systems into Google <a href="https://www.google.com/about/products/">products</a> and <a href="https://ai.google/tools/">platforms</a>. <br>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://1.bp.blogspot.com/-bvrC_8FD5QU/WvDSR37K6lI/AAAAAAAACqs/Ha6uPQwxf3IXOUl4_Ybm2KZjS0Bc3EDpQCLcBGAs/s1600/thinking_v2.gif" imageanchor="1" style="margin-left: 1em; margin-right: 1em;" target="_blank"><img border="0" data-original-height="512" data-original-width="512" height="200" src="./blog_files/thinking_v2.gif" width="200"></a></div>
To better reflect this commitment, we’re unifying our efforts under “Google AI”, which encompasses all the state-of-the-art research happening across Google. As part of this, we have expanded the <a href="http://ai.google/">Google AI website</a>, and are renaming our existing Google Research channels, including this blog and the affiliated <a href="https://twitter.com/googleai">Twitter</a> and <a href="https://plus.google.com/+GoogleAI">Google+</a> channels, to Google AI. And if you’re looking for information that existed on research.google.com or the affiliated social channels, don’t fret, it’s all still there. Any links to previous Google Research website content, blog posts or tweets will redirect appropriately.<br>
<br>
The Google AI channels will continue to showcase the breadth of Google research, innovation and publications, in addition to a lot more new and exciting content to come. We encourage you to explore!  We look forward to continuing to bring you the latest updates and results from Google, in AI and across many other areas of research.
<span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/05/introducing-google-ai.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Introducing Google AI&amp;url=http://ai.googleblog.com/2018/05/introducing-google-ai.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/05/introducing-google-ai.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div>
<div class="comment-container">
<i class="comment-img material-icons">
                            
                          </i>
<span class="cmt_count_iframe_holder" data-count="0" data-onclick="" data-post-url="http://ai.googleblog.com/2018/05/introducing-google-ai.html" data-url="http://ai.googleblog.com/2018/05/introducing-google-ai.html" style="color: rgb(65, 132, 243); text-indent: 0px; margin: 0px; padding: 0px; background: transparent; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: text-top; display: inline-block; width: 76px; height: 17px;" id="_id_692622171559976270"><iframe ng-non-bindable="" frameborder="0" hspace="0" marginheight="0" marginwidth="0" scrolling="no" style="position: static; top: 0px; width: 76px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 17px;" tabindex="0" vspace="0" width="100%" id="I6_1527709895715" name="I6_1527709895715" src="./blog_files/commentcount(6).html" data-gapiattached="true" title="&lt;style&gt;body {background-color: transparent;}&lt;/style&gt;&lt;style&gt;a, span {font-family:&#39;Roboto&#39;;font-size: 14px;color: #4184f3;display: block;}&lt;/style&gt;
  &lt;script&gt;
    function reportClick() {
      var iframer = window.iframes.iframer;
      if (iframer.onclick) {
        iframer.onclick();
      }
    }
  &lt;/script&gt;
  &lt;div id=&quot;widget_bounds&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;reportClick();&quot;&gt;65 comments&lt;/a&gt;&lt;/div&gt;"></iframe></span>
</div>
<div class="post-footer">
<a href="https://plus.google.com/112374322230920073195" rel="author" style="display:none;">
                        Google
                      </a>
<div class="label-footer">
<span class="labels-caption">
Labels:
</span>
<span class="labels">
<a class="label" href="http://ai.googleblog.com/search/label/AI" rel="tag">
AI
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Research" rel="tag">
Research
</a>
</span>
</div>
</div>
</div>
<div class="post" data-id="6187950619443532987" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/05/the-question-of-quantum-supremacy.html" itemprop="url" title="The Question of Quantum Supremacy">
The Question of Quantum Supremacy
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Friday, May 4, 2018
</span>
</div>
</div>
<div class="post-body">
<div class="post-content post-summary"><span class="byline-author">Posted by Sergio Boixo, Research Scientist and Theory Team Lead, and Charles Neill, Quantum Electronics Engineer, Quantum A.I. Lab</span><br>
 <br>
 <a href="https://en.wikipedia.org/wiki/Quantum_technology">Quantum computing</a> integrates the two largest technological revolutions of the last half century ...<a href="http://ai.googleblog.com/2018/05/the-question-of-quantum-supremacy.html" itemprop="url" title="The Question of Quantum Supremacy" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Sergio Boixo, Research Scientist and Theory Team Lead, and Charles Neill, Quantum Electronics Engineer, Quantum A.I. Lab</span><br>
<br>
<a href="https://en.wikipedia.org/wiki/Quantum_technology">Quantum computing</a> integrates the two largest technological revolutions of the last half century, <a href="https://en.wikipedia.org/wiki/Information_technology">information technology</a> and <a href="https://en.wikipedia.org/wiki/Quantum_mechanics">quantum mechanics</a>. If we compute using the rules of quantum mechanics, instead of binary logic, some <a href="https://en.wikipedia.org/wiki/Hubbard_model">intractable</a> <a href="https://en.wikipedia.org/wiki/Nitrogen_fixation">computational tasks</a> become feasible. An important goal in the pursuit of a universal quantum computer is the determination of the smallest computational task that is prohibitively hard for today’s classical computers. This crossover point is known as the “quantum supremacy” frontier, and is a critical step on the path to more powerful and useful computations.<br>
<br>
In “<a href="https://www.nature.com/articles/s41567-018-0124-x">Characterizing quantum supremacy in near-term devices</a>” published in Nature Physics (arXiv <a href="https://arxiv.org/abs/1608.00263">here</a>), we present the theoretical foundation for a practical demonstration of quantum supremacy in near-term devices. It proposes the task of sampling bit-strings from the output of random quantum circuits, which can be thought of as the “hello world” program for quantum computers. The upshot of the argument is that the output of random chaotic systems (think <a href="https://en.wikipedia.org/wiki/Butterfly_effect">butterfly effect</a>) become very quickly harder to predict the longer they run. If one makes a random, chaotic <a href="https://en.wikipedia.org/wiki/Qubit">qubit</a> system and examines how long a classical system would take to emulate it, one gets a good measure of when a quantum computer could outperform a classical one. <a href="https://arxiv.org/abs/1803.04402">Arguably</a>, this is the strongest theoretical proposal to prove an <a href="https://arxiv.org/abs/1011.3245">exponential</a> <a href="https://arxiv.org/abs/1504.07999">separation</a> between the <a href="https://arxiv.org/abs/1612.05903">computational</a> power of classical and quantum computers.  <br>
<br>
Determining where exactly the quantum supremacy frontier lies for sampling random quantum circuits has rapidly become an exciting area of research. On one hand, <a href="https://arxiv.org/abs/1710.05867">improvements</a> in <a href="https://arxiv.org/abs/1712.05384">classical</a> <a href="http://arxiv.org/abs/1802.06952">algorithms</a> to <a href="https://arxiv.org/abs/1804.04797">simulate quantum circuits</a>&nbsp;aim to <a href="https://arxiv.org/abs/1805.01450">increase the size</a>&nbsp;of the quantum circuits required to establish quantum supremacy. This forces an experimental quantum device with a sufficiently large number of qubits and low enough error rates to implement circuits of sufficient depth (i.e the number of layers of gates in the circuit) to achieve supremacy. On the other hand, we now understand better how the particular choice of the quantum gates used to build random quantum circuits affects the <a href="http://arxiv.org/abs/1804.04797">simulation cost</a>, leading to improved benchmarks for near-term quantum supremacy (available for download <a href="https://github.com/sboixo/GRCS">here</a>), which are in some cases quadratically more expensive to simulate classically than the original proposal.<br>
<br>
Sampling from random quantum circuits is an excellent calibration benchmark for quantum computers, which we call <i>cross-entropy benchmarking</i>. A successful quantum supremacy experiment with random circuits would demonstrate the basic building blocks for a large-scale <a href="https://journals.aps.org/pra/abstract/10.1103/PhysRevA.86.032324">fault-tolerant</a> quantum computer. Furthermore, quantum physics has not yet been tested for <a href="https://arxiv.org/abs/1203.5813">highly complex</a> quantum states such as this. <br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-ZOtsQgZroZc/WuyXsilCW1I/AAAAAAAACo0/BULrTsPCQioAsgK2KhY99icD1bj5cHd-QCLcBGAs/s1600/image1.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="1165" data-original-width="1600" height="466" src="./blog_files/image1(1).jpg" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;"><a href="https://pdfs.semanticscholar.org/650c/3fa2a231cd77cf3d882e1659ee14175c01d5.pdf">Space-time volume</a> of a quantum circuit computation. The computational cost for quantum simulation increases with the volume of the quantum circuit, and in general grows exponentially with the number of qubits and the circuit depth. For asymmetric grids of qubits, the computational space-time volume grows slower with depth than for symmetric grids, and can result in circuits exponentially <a href="https://arxiv.org/abs/1712.05384">easier to simulate</a>.</td></tr>
</tbody></table>In “<a href="http://science.sciencemag.org/content/360/6385/195">A blueprint for demonstrating quantum supremacy with superconducting qubits</a>” (arXiv <a href="https://arxiv.org/abs/1709.06678">here</a>), we illustrate a blueprint towards quantum supremacy and experimentally demonstrate a proof-of-principle version for the first time.  In the paper, we discuss two key ingredients for quantum supremacy: exponential complexity and accurate computations. We start by running algorithms on subsections of the device ranging from 5 to 9 qubits.  We find that the classical simulation cost grows exponentially with the number of qubits.  These results are intended to provide a clear example of the exponential power of these devices.  Next, we use <i>cross-entropy benchmarking</i> to compare our results against that of an ordinary computer and show that our computations are highly accurate.  In fact, the error rate is low enough to achieve quantum supremacy with a larger quantum processor. <br>
<br>
Beyond achieving quantum supremacy, a quantum platform should offer clear applications.  In our paper, we apply our algorithms towards computational problems in quantum statistical-mechanics using complex multi-qubit gates (as opposed to the two-qubit gates designed for a <a href="https://research.googleblog.com/2018/03/a-preview-of-bristlecone-googles-new.html">digital quantum processor</a> with <a href="https://research.googleblog.com/2015/03/a-step-closer-to-quantum-computation.html">surface code error correction</a>). We show that our devices can be used to study fundamental properties of materials, e.g. microscopic differences between metals and insulators.  By extending these results to next-generation devices with ~50 qubits, we hope to answer scientific questions that are beyond the capabilities of any other computing platform.<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://4.bp.blogspot.com/-fb2grNdr0Zo/WuyYB808xwI/AAAAAAAACo8/s7NtNVI93LEsnMNZ5eP7NzwDtacnAa35gCLcBGAs/s1600/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="1119" data-original-width="1600" height="278" src="./blog_files/image2(3).png" width="400"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Photograph of two gmon superconducting qubits and their tunable coupler developed by Charles Neill and Pedram Roushan.</td></tr>
</tbody></table>These two publications introduce a realistic proposal for near-term quantum supremacy, and demonstrate a proof-of-principle version for the first time. We will continue to decrease the error rates and increase the number of qubits in <a href="https://research.googleblog.com/2018/03/a-preview-of-bristlecone-googles-new.html">quantum processors</a> to reach the quantum supremacy frontier, and to develop <a href="https://research.googleblog.com/2017/10/announcing-openfermion-open-source.html">quantum algorithms</a> for useful near-term <a href="https://research.googleblog.com/2018/03/reformulating-chemistry-for-more.html">applications</a>.
<span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/05/the-question-of-quantum-supremacy.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:The Question of Quantum Supremacy&amp;url=http://ai.googleblog.com/2018/05/the-question-of-quantum-supremacy.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/05/the-question-of-quantum-supremacy.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div>
<div class="comment-container">
<i class="comment-img material-icons">
                            
                          </i>
<span class="cmt_count_iframe_holder" data-count="0" data-onclick="" data-post-url="http://ai.googleblog.com/2018/05/the-question-of-quantum-supremacy.html" data-url="http://ai.googleblog.com/2018/05/the-question-of-quantum-supremacy.html" style="color: rgb(65, 132, 243); text-indent: 0px; margin: 0px; padding: 0px; background: transparent; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: text-top; display: inline-block; width: 69px; height: 17px;" id="_id_76377254731308457"><iframe ng-non-bindable="" frameborder="0" hspace="0" marginheight="0" marginwidth="0" scrolling="no" style="position: static; top: 0px; width: 69px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 17px;" tabindex="0" vspace="0" width="100%" id="I7_1527709895721" name="I7_1527709895721" src="./blog_files/commentcount(7).html" data-gapiattached="true" title="&lt;style&gt;body {background-color: transparent;}&lt;/style&gt;&lt;style&gt;a, span {font-family:&#39;Roboto&#39;;font-size: 14px;color: #4184f3;display: block;}&lt;/style&gt;
  &lt;script&gt;
    function reportClick() {
      var iframer = window.iframes.iframer;
      if (iframer.onclick) {
        iframer.onclick();
      }
    }
  &lt;/script&gt;
  &lt;div id=&quot;widget_bounds&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;reportClick();&quot;&gt;6 comments&lt;/a&gt;&lt;/div&gt;"></iframe></span>
</div>
<div class="post-footer">
<a href="https://plus.google.com/112374322230920073195" rel="author" style="display:none;">
                        Google
                      </a>
<div class="label-footer">
<span class="labels-caption">
Labels:
</span>
<span class="labels">
<a class="label" href="http://ai.googleblog.com/search/label/Algorithms" rel="tag">
Algorithms
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Computer%20Science" rel="tag">
Computer Science
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Physics" rel="tag">
Physics
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Publications" rel="tag">
Publications
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Quantum%20AI" rel="tag">
Quantum AI
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Quantum%20Computing" rel="tag">
Quantum Computing
</a>
</span>
</div>
</div>
</div>
<div class="post" data-id="2369464916067160154" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html" itemprop="url" title="Announcing Open Images V4 and the ECCV 2018 Open Images Challenge">
Announcing Open Images V4 and the ECCV 2018 Open Images Challenge
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Monday, April 30, 2018
</span>
</div>
</div>
<div class="post-body">
<div class="post-content post-summary"><span class="byline-author">Posted by Vittorio Ferrari, Research Scientist, Machine Perception</span><br>
 <br>
In 2016, we introduced  <a href="https://research.googleblog.com/2016/09/introducing-open-images-dataset.html">Open Images</a>, a collaborative release of ~9 million images annotated with labels spanning thousands of object categories. Since its initial release, we've been hard at work ...<a href="http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html" itemprop="url" title="Announcing Open Images V4 and the ECCV 2018 Open Images Challenge" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Vittorio Ferrari, Research Scientist, Machine Perception</span><br>
<br>
In 2016, we introduced <a href="https://research.googleblog.com/2016/09/introducing-open-images-dataset.html">Open Images</a>, a collaborative release of ~9 million images annotated with labels spanning thousands of object categories. Since its initial release, we've been hard at work <a href="https://research.googleblog.com/2017/07/an-update-to-open-images-now-with.html">updating</a> and <a href="https://twitter.com/googleresearch/status/932670010740195328">refining</a> the dataset, in order to provide a useful resource for the computer vision community to develop new models.<br>
<br>
Today, we are happy to announce <a href="https://storage.googleapis.com/openimages/web/index.html">Open Images V4</a>, containing 15.4M bounding-boxes for 600 categories on 1.9M images, making it the <i>largest existing dataset</i> with object location annotations. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (8 per image on average; <a href="https://storage.googleapis.com/openimages/web/visualizer/index.html">visualizer</a>).<br>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-beTVIi_DQaU/WucuwvFvDAI/AAAAAAAACn8/4kiNhkI1BVUWsjxeYbG1LLAqWJE2QIKEwCLcBGAs/s1600/f1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;" target="_blank"><img border="0" data-original-height="1023" data-original-width="1600" height="408" src="./blog_files/f1(1).png" width="640"></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Annotated images from the Open Images dataset. Left: <a href="https://www.flickr.com/photos/rhysasplundh/5738556102">Mark Paul Gosselaar plays the guitar</a> by <a href="https://www.flickr.com/people/rhysasplundh/">Rhys A.</a> Right: <a href="https://www.flickr.com/photos/psd/15223644448">Civilization</a> by <a href="https://www.flickr.com/people/psd/">Paul Downey</a>. Both images used under <a href="https://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a> license.</td></tr>
</tbody></table>In conjunction with this release, we are also introducing the <a href="https://storage.googleapis.com/openimages/web/challenge.html">Open Images Challenge</a>, a new object detection challenge to be held at the <a href="https://eccv2018.org/">2018 European Conference on Computer Vision</a> (ECCV 2018). The Open Images Challenge follows in the tradition of <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">PASCAL VOC</a>, <a href="https://www.kaggle.com/c/imagenet-object-localization-challenge">ImageNet</a> and <a href="https://places-coco2017.github.io/">COCO</a>, but at an unprecedented scale.<br>
<br>
This challenge is unique in several ways:<br>
<ul><li>12.2M bounding-box annotations for 500 categories on 1.7M training images,</li>
<li>A  broader range of categories than previous detection challenges, including new objects such as “fedora” and “snowman”.</li>
<li>In addition to the object detection main track, the challenge includes a <i>Visual Relationship Detection</i> track, on detecting pairs of objects in particular relations, e.g. “woman playing guitar”.</li>
</ul>The training set is available now. A test set of 100k images will be released on July 1st 2018 by Kaggle. Deadline for submission of results is on September 1st 2018. We hope that the very large training set will stimulate research into more sophisticated detection models that will exceed current state-of-the-art performance, and that the 500 categories will enable a more precise assessment of where different detectors perform best. Furthermore, having a large set of images with many objects annotated enables to explore Visual Relationship Detection, which is a hot emerging topic with a growing sub-community.<br>
<br>
In addition to the above, Open Images V4 also contains 30.1M human-verified image-level labels for 19,794 categories, which are not part of the Challenge. The dataset includes 5.5M image-level labels generated by tens of thousands of users from all over the world at <a href="https://crowdsource.google.com/imagelabeler">crowdsource.google.com</a>. 
<span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Announcing Open Images V4 and the ECCV 2018 Open Images Challenge&amp;url=http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div>
<div class="comment-container">
<i class="comment-img material-icons">
                            
                          </i>
<span class="cmt_count_iframe_holder" data-count="0" data-onclick="" data-post-url="http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html" data-url="http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html" style="color: rgb(65, 132, 243); text-indent: 0px; margin: 0px; padding: 0px; background: transparent; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: text-top; display: inline-block; width: 69px; height: 17px;" id="_id_855890732414053310"><iframe ng-non-bindable="" frameborder="0" hspace="0" marginheight="0" marginwidth="0" scrolling="no" style="position: static; top: 0px; width: 69px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 17px;" tabindex="0" vspace="0" width="100%" id="I8_1527709895735" name="I8_1527709895735" src="./blog_files/commentcount(8).html" data-gapiattached="true" title="&lt;style&gt;body {background-color: transparent;}&lt;/style&gt;&lt;style&gt;a, span {font-family:&#39;Roboto&#39;;font-size: 14px;color: #4184f3;display: block;}&lt;/style&gt;
  &lt;script&gt;
    function reportClick() {
      var iframer = window.iframes.iframer;
      if (iframer.onclick) {
        iframer.onclick();
      }
    }
  &lt;/script&gt;
  &lt;div id=&quot;widget_bounds&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; onclick=&quot;reportClick();&quot;&gt;2 comments&lt;/a&gt;&lt;/div&gt;"></iframe></span>
</div>
<div class="post-footer">
<div class="cmt_iframe_holder" data-href="http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html" data-viewtype="FILTERED_POSTMOD"></div>
<a href="https://plus.google.com/112374322230920073195" rel="author" style="display:none;">
                        Google
                      </a>
<div class="label-footer">
<span class="labels-caption">
Labels:
</span>
<span class="labels">
<a class="label" href="http://ai.googleblog.com/search/label/Computer%20Vision" rel="tag">
Computer Vision
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/CVPR" rel="tag">
CVPR
</a>

                                ,
                              
<a class="label" href="http://ai.googleblog.com/search/label/Machine%20Learning" rel="tag">
Machine Learning
</a>
</span>
</div>
</div>
</div>
<div class="post" data-id="6538246066839107709" itemscope="" itemtype="http://schema.org/BlogPosting">
<h2 class="title" itemprop="name">
<a href="http://ai.googleblog.com/2018/04/google-at-iclr-2018.html" itemprop="url" title="Google at ICLR 2018">
Google at ICLR 2018
</a>
</h2>
<div class="post-header">
<div class="published">
<span class="publishdate" itemprop="datePublished">
Sunday, April 29, 2018
</span>
</div>
</div>
<div class="post-body">
<div class="post-content post-summary"><span class="byline-author">Posted by Jeff Dean, Google Senior Fellow, Head of Google Research and Machine Intelligence</span><br><br>This week, Vancouver, Canada hosts the  <a href="https://iclr.cc/archive/www/2018.html">6th International Conference on Learning Representations</a> (ICLR 2018), a conference focused on how one can learn meaningful and useful representations of data for ...<a href="http://ai.googleblog.com/2018/04/google-at-iclr-2018.html" itemprop="url" title="Google at ICLR 2018" class="read-more">Read More</a></div><div class="post-content post-original" itemprop="articleBody">
                          <span class="byline-author">Posted by Jeff Dean, Google Senior Fellow, Head of Google Research and Machine Intelligence</span><br><br>This week, Vancouver, Canada hosts the <a href="https://iclr.cc/archive/www/2018.html">6th International Conference on Learning Representations</a> (ICLR 2018), a conference focused on how one can learn meaningful and useful representations of data for <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>.  ICLR includes conference and workshop tracks, with invited talks along with oral and poster presentations of some of the latest research on deep learning, metric learning, kernel learning, compositional models, non-linear structured prediction, and issues regarding non-convex optimization.<br><br>At the forefront of innovation in cutting-edge technology in <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural networks</a> and <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>, Google focuses on both theory and application, developing learning approaches to understand and generalize. As Platinum Sponsor of ICLR 2018, Google will have a strong presence with over 130 researchers attending, contributing to and learning from the broader academic research community by presenting papers and posters, in addition to participating on organizing committees and in workshops.<br><br>If you are attending ICLR 2018, we hope you'll stop by our booth and chat with our researchers about the projects and opportunities at Google that go into solving interesting problems for billions of people. You can also learn more about our research being presented at ICLR 2018 in the list below (Googlers highlighted in <span style="color: #3d85c6;">blue</span>)<br><br><u><b>Senior Program Chair:</b></u><br><span style="color: #3d85c6;"><i>Tara Sainath</i></span><br><br><u><b>Steering Committee includes:</b></u><br><span style="color: #3d85c6;"><i>Hugo Larochelle</i></span><br><br><b><u>Oral Contributions</u></b><br><a href="https://openreview.net/pdf?id=HkL7n1-0b">Wasserstein Auto-Encoders</a><br><i>Ilya Tolstikhin, <span style="color: #3d85c6;">Olivier Bousquet</span>, <span style="color: #3d85c6;">Sylvain Gelly</span>, Bernhard Scholkopf</i><br><br><a href="https://openreview.net/pdf?id=ryQu7f-RZ">On the Convergence of Adam and Beyond</a> <b>(Best Paper Award)</b><br><i><span style="color: #3d85c6;">Sashank J. Reddi</span>, <span style="color: #3d85c6;">Satyen Kale</span>, <span style="color: #3d85c6;">Sanjiv Kumar</span></i><br><br><a href="https://openreview.net/pdf?id=S1CChZ-CZ">Ask the Right Questions: Active Question Reformulation with Reinforcement Learning</a><br><i><span style="color: #3d85c6;">Christian Buck</span>,<span style="color: #3d85c6;">&nbsp;Jannis Bulian</span>,<span style="color: #3d85c6;">&nbsp;Massimiliano Ciaramita</span>,<span style="color: #3d85c6;">&nbsp;Wojciech Gajewski</span>,<span style="color: #3d85c6;">&nbsp;Andrea Gesmundo</span>,<span style="color: #3d85c6;">&nbsp;Neil Houlsby</span>,<span style="color: #3d85c6;">&nbsp;Wei Wang</span></i><br><br><a href="https://openreview.net/pdf?id=rkRwGg-0Z">Beyond Word Importance: Contextual Decompositions to Extract Interactions from LSTMs</a><br><i>W. James Murdoch, <span style="color: #3d85c6;">Peter J. Liu</span>, Bin Yu</i><br><br><b><u>Conference Posters</u></b><br><a href="https://openreview.net/pdf?id=BkUp6GZRW">Boosting the Actor with Dual Critic</a><br><i>Bo Dai, Albert Shaw, Niao He, <span style="color: #3d85c6;">Lihong Li</span>, Le Song</i><br><br><a href="https://openreview.net/pdf?id=ByOExmWAb">MaskGAN: Better Text Generation via Filling in the _______</a><br><i><span style="color: #3d85c6;">William Fedus</span>,<span style="color: #3d85c6;">&nbsp;Ian Goodfellow</span>,<span style="color: #3d85c6;">&nbsp;Andrew M. Dai</span></i><br><br><a href="https://openreview.net/pdf?id=rkZB1XbRZ">Scalable Private Learning with PATE</a><br><i>Nicolas Papernot, Shuang Song, <span style="color: #3d85c6;">Ilya Mironov</span>,<span style="color: #3d85c6;">&nbsp;Ananth Raghunathan</span>,<span style="color: #3d85c6;">&nbsp;Kunal Talwar</span>,<span style="color: #3d85c6;">&nbsp;Ulfar Erlingsson</span></i><br><br><a href="https://openreview.net/pdf?id=SkhQHMW0W">Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training</a><br><i>Yujun Lin, <span style="color: #3d85c6;">Song Han</span>, Huizi Mao, Yu Wang, William J. Dally</i><br><br><a href="https://openreview.net/pdf?id=rJNpifWAb">Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches</a><br><i>Yeming Wen, Paul Vicol, Jimmy Ba, <span style="color: #3d85c6;">Dustin Tran</span>, Roger Grosse</i><br><br><a href="https://openreview.net/pdf?id=Sy8XvGb0-">Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models</a><br><i><span style="color: #3d85c6;">Adam Roberts</span>,<span style="color: #3d85c6;">&nbsp;Jesse Engel</span>,<span style="color: #3d85c6;">&nbsp;Matt Hoffman</span></i><br><br><a href="https://openreview.net/pdf?id=HyRnez-RW">Multi-Mention Learning for Reading Comprehension with Neural Cascades</a><br><i>Swabha Swayamdipta, <span style="color: #3d85c6;">Ankur P. Parikh</span>, <span style="color: #3d85c6;">Tom Kwiatkowski</span></i><br><br><a href="https://openreview.net/pdf?id=B14TlG-RW">QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension</a><br><i>Adams Wei Yu, <span style="color: #3d85c6;">David Dohan</span>,<span style="color: #3d85c6;">&nbsp;Thang Luong</span>,<span style="color: #3d85c6;">&nbsp;Rui Zhao</span>,<span style="color: #3d85c6;">&nbsp;Kai Chen</span>,<span style="color: #3d85c6;">&nbsp;Mohammad Norouzi</span>,<span style="color: #3d85c6;">&nbsp;Quoc V. Le</span></i><br><br><a href="https://openreview.net/pdf?id=HJC2SzZCW">Sensitivity and Generalization in Neural Networks: An Empirical Study</a><br><i><span style="color: #3d85c6;">Roman Novak</span>,<span style="color: #3d85c6;">&nbsp;Yasaman Bahri</span>,<span style="color: #3d85c6;">&nbsp;Daniel A. Abolafia</span>,<span style="color: #3d85c6;">&nbsp;Jeffrey Pennington</span>,<span style="color: #3d85c6;">&nbsp;Jascha Sohl-Dickstein</span></i><br><br><a href="https://openreview.net/pdf?id=H1mCp-ZRZ">Action-dependent Control Variates for Policy Optimization via Stein Identity</a><br><i>Hao Liu, Yihao Feng, Yi Mao, <span style="color: #3d85c6;">Dengyong Zhou</span>, Jian Peng, Qiang Liu</i><br><br><a href="https://openreview.net/pdf?id=rJvJXZb0W">An Efficient Framework for Learning Sentence Representations</a><br><i>Lajanugen Logeswaran, <span style="color: #3d85c6;">Honglak Lee </span></i><br><br><a href="https://openreview.net/pdf?id=B1X0mzZCW">Fidelity-Weighted Learning</a><br><i>Mostafa Dehghani, Arash Mehrjou, <span style="color: #3d85c6;">Stephan Gouws</span>, Jaap Kamps, Bernhard Schölkopf </i><br><br><a href="https://openreview.net/pdf?id=Hyg0vbWC-">Generating Wikipedia by Summarizing Long Sequences</a> <br><i><span style="color: #3d85c6;">Peter J. Liu</span>,<span style="color: #3d85c6;">&nbsp;Mohammad Saleh</span>,<span style="color: #3d85c6;">&nbsp;Etienne Pot</span>,<span style="color: #3d85c6;">&nbsp;Ben Goodrich</span>,<span style="color: #3d85c6;">&nbsp;Ryan Sepassi</span>,<span style="color: #3d85c6;">&nbsp;Lukasz Kaiser</span>,<span style="color: #3d85c6;">&nbsp;Noam Shazeer</span></i><br><br><a href="https://openreview.net/pdf?id=HJWLfGWRb">Matrix Capsules with EM Routing</a> <br><i><span style="color: #3d85c6;">Geoffrey Hinton</span>,<span style="color: #3d85c6;">&nbsp;Sara Sabour</span>,<span style="color: #3d85c6;">&nbsp;Nicholas Frosst </span></i><br><br><a href="https://openreview.net/pdf?id=Skw0n-W0Z">Temporal Difference Models: Model-Free Deep RL for Model-Based Control</a><br><i>Sergey Levine, <span style="color: #3d85c6;">Shixiang Gu</span>, Murtaza Dalal, Vitchyr Pong </i><br><br><a href="https://openreview.net/pdf?id=B1EA-M-0Z">Deep Neural Networks as Gaussian Processes</a> <br><i><span style="color: #3d85c6;">Jaehoon Lee</span>,<span style="color: #3d85c6;">&nbsp;Yasaman Bahri</span>,<span style="color: #3d85c6;">&nbsp;Roman Novak</span>,<span style="color: #3d85c6;">&nbsp;Samuel L. Schoenholz</span>,<span style="color: #3d85c6;">&nbsp;Jeffrey Pennington</span>,<span style="color: #3d85c6;">&nbsp;Jascha Sohl-Dickstein</span></i><br><br><a href="https://openreview.net/pdf?id=ByQpn1ZA-">Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence at Every Step</a><br><i><span style="color: #3d85c6;">William Fedus</span>, Mihaela Rosca, Balaji Lakshminarayanan, <span style="color: #3d85c6;">Andrew M. Dai</span>, Shakir Mohamed, <span style="color: #3d85c6;">Ian Goodfellow </span></i><br><br><a href="https://openreview.net/pdf?id=HJJ23bW0b">Initialization Matters: Orthogonal Predictive State Recurrent Neural Networks</a> <br><i><span style="color: #3d85c6;">Krzysztof Choromanski</span>, Carlton Downey, Byron Boots</i><br><br><a href="https://openreview.net/pdf?id=BJ0hF1Z0b">Learning Differentially Private Recurrent Language Models</a> <br><i><span style="color: #3d85c6;">H. Brendan McMahan</span>,<span style="color: #3d85c6;">&nbsp;Daniel Ramage</span>,<span style="color: #3d85c6;">&nbsp;Kunal Talwar</span>,<span style="color: #3d85c6;">&nbsp;Li Zhang</span></i><br><br><a href="https://openreview.net/pdf?id=Byt3oJ-0W">Learning Latent Permutations with Gumbel-Sinkhorn Networks</a> <br><i>Gonzalo Mena, <span style="color: #3d85c6;">David Belanger</span>, Scott Linderman, <span style="color: #3d85c6;">Jasper Snoek </span></i><br><br><a href="https://openreview.net/pdf?id=S1vuO-bCW">Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning</a> <br><i><span style="color: #3d85c6;">Benjamin Eysenbach</span>,<span style="color: #3d85c6;">&nbsp;Shixiang Gu</span>,<span style="color: #3d85c6;">&nbsp;Julian Ibarz</span>,&nbsp;<span style="color: #3d85c6;">Sergey Levine</span></i><br><br><a href="https://openreview.net/pdf?id=HJcSzz-CZ">Meta-Learning for Semi-Supervised Few-Shot Classification</a><br><i>Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, <span style="color: #3d85c6;">Kevin Swersky</span>, Josh Tenenbaum, <span style="color: #3d85c6;">Hugo Larochelle</span>, Richard Zemel</i><br><br><a href="https://openreview.net/pdf?id=S18Su--CW">Thermometer Encoding: One Hot Way to Resist Adversarial Examples</a><br><i><span style="color: #3d85c6;">Jacob Buckman</span>,<span style="color: #3d85c6;">&nbsp;Aurko Roy</span>,<span style="color: #3d85c6;">&nbsp;Colin Raffel</span>,<span style="color: #3d85c6;">&nbsp;Ian Goodfellow</span></i><br><br><a href="https://openreview.net/pdf?id=Hkc-TeZ0W">A Hierarchical Model for Device Placement</a> <br><i><span style="color: #3d85c6;">Azalia Mirhoseini</span>,<span style="color: #3d85c6;">&nbsp;Anna Goldie</span>,<span style="color: #3d85c6;">&nbsp;Hieu Pham</span>,<span style="color: #3d85c6;">&nbsp;Benoit Steiner</span>,<span style="color: #3d85c6;">&nbsp;Quoc V. Le</span>,&nbsp;<span style="color: #3d85c6;">Jeff Dean </span></i><br><br><a href="https://openreview.net/pdf?id=Hko85plCW">Monotonic Chunkwise Attention</a><br><i><span style="color: #3d85c6;">Chung-Cheng Chiu</span>,<span style="color: #3d85c6;">&nbsp;Colin Raffel</span></i><br><br><a href="https://openreview.net/pdf?id=ryiAv2xAZ">Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples</a><br><i>Kimin Lee, <span style="color: #3d85c6;">Honglak Lee</span>, Kibok Lee, Jinwoo Shin</i><br><br><a href="https://openreview.net/pdf?id=HyrCWeWCb">Trust-PCL: An Off-Policy Trust Region Method for Continuous Control</a><br><i><span style="color: #3d85c6;">Ofir Nachum</span>,<span style="color: #3d85c6;">&nbsp;Mohammad Norouzi</span>,<span style="color: #3d85c6;">&nbsp;Kelvin Xu</span>,<span style="color: #3d85c6;">&nbsp;Dale Schuurmans</span></i><br><br><a href="https://openreview.net/pdf?id=rkZvSe-RZ">Ensemble Adversarial Training: Attacks and Defenses</a> <br><i>Florian Tramèr, <span style="color: #3d85c6;">Alexey Kurakin</span>,<span style="color: #3d85c6;">&nbsp;Nicolas Papernot</span>,<span style="color: #3d85c6;">&nbsp;Ian Goodfellow</span>, Dan Boneh, Patrick McDaniel </i><br><br><a href="https://openreview.net/pdf?id=rk49Mg-CW">Stochastic Variational Video Prediction</a><br><i>Mohammad Babaeizadeh, Chelsea Finn, <span style="color: #3d85c6;">Dumitru</span> Erhan, Roy Campbell, <span style="color: #3d85c6;">Sergey Levine</span></i><br><br><a href="https://openreview.net/pdf?id=S1jBcueAb">Depthwise Separable Convolutions for Neural Machine Translation</a><br><i><span style="color: #3d85c6;">Lukasz Kaiser</span>, Aidan N. Gomez, <span style="color: #3d85c6;">Francois Chollet</span></i><br><br><a href="https://openreview.net/pdf?id=B1Yy1BxCZ">Don’t Decay the Learning Rate, Increase the Batch Size </a><br><i><span style="color: #3d85c6;">Samuel L. Smith</span>,<span style="color: #3d85c6;">&nbsp;Pieter-Jan Kindermans</span>,<span style="color: #3d85c6;">&nbsp;Chris Ying</span>,<span style="color: #3d85c6;">&nbsp;Quoc V. Le</span></i><br><br><a href="https://openreview.net/pdf?id=HkCsm6lRb">Generative Models of Visually Grounded Imagination</a><br><i>Ramakrishna Vedantam, <span style="color: #3d85c6;">Ian Fischer</span>,<span style="color: #3d85c6;">&nbsp;Jonathan Huang</span>,<span style="color: #3d85c6;">&nbsp;Kevin Murphy</span></i><br><br><a href="https://openreview.net/pdf?id=rkr1UDeC-">Large Scale Distributed Neural Network Training through Online Distillation</a> <br><i><span style="color: #3d85c6;">Rohan Anil</span>, Gabriel Pereyra, <span style="color: #3d85c6;">Alexandre Passos</span>,<span style="color: #3d85c6;">&nbsp;Robert Ormandi</span>,<span style="color: #3d85c6;">&nbsp;George E. Dahl</span>,<span style="color: #3d85c6;">&nbsp;Geoffrey E. Hinton</span></i><br><br><a href="https://openreview.net/pdf?id=HJhIM0xAW">Learning a Neural Response Metric for Retinal Prosthesis</a><br><i><span style="color: #3d85c6;">Nishal P. Shah</span>, Sasidhar Madugula, Alan Litke, Alexander Sher, EJ Chichilnisky, <span style="color: #3d85c6;">Yoram Singer,</span> <span style="color: #3d85c6;">Jonathon Shlens</span></i><br><br><a href="https://openreview.net/pdf?id=rkLyJl-0-">Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks</a><br><i><span style="color: #3d85c6;">Shankar Krishnan</span>,<span style="color: #3d85c6;">&nbsp;Ying Xiao</span>,<span style="color: #3d85c6;">&nbsp;Rif A. Saurous</span></i><br><br><a href="https://openreview.net/pdf?id=Hy6GHpkCW">A Neural Representation of Sketch Drawings</a><br><i><span style="color: #3d85c6;">David Ha</span>,&nbsp;<span style="color: #3d85c6;">Douglas Eck</span></i><br><br><a href="https://openreview.net/pdf?id=SyYe6k-CW">Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling</a> <br><i><span style="color: #3d85c6;">Carlos Riquelme</span>,<span style="color: #3d85c6;">&nbsp;George Tucker</span>,<span style="color: #3d85c6;">&nbsp;Jasper Snoek</span></i><br><br><a href="https://openreview.net/pdf?id=B1n8LexRZ">Generalizing Hamiltonian Monte Carlo with Neural Networks</a><br><i>Daniel Levy, <span style="color: #3d85c6;">Matthew D. Hoffman</span>,&nbsp;<span style="color: #3d85c6;">Jascha Sohl-Dickstein</span></i><br><br><a href="https://openreview.net/pdf?id=H1Xw62kRZ">Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis</a><br><i>Rudy Bunel, Matthew Hausknecht, <span style="color: #3d85c6;">Jacob Devlin</span>, Rishabh Singh, Pushmeet Kohli</i><br><br><a href="https://openreview.net/pdf?id=Hk9Xc_lR-"> On the Discrimination-Generalization Tradeoff in GANs</a><br><i>Pengchuan Zhang, Qiang Liu, <span style="color: #3d85c6;">Dengyong Zhou</span>, Tao Xu, Xiaodong He</i><br><br><a href="https://openreview.net/pdf?id=BJij4yg0Z">A Bayesian Perspective on Generalization and Stochastic Gradient Descent</a><br><i><span style="color: #3d85c6;">Samuel L. Smith</span>,<span style="color: #3d85c6;">&nbsp;Quoc V. Le </span></i><br><br><a href="https://openreview.net/pdf?id=Hkn7CBaTW">Learning how to Explain Neural Networks: PatternNet and PatternAttribution</a><br><i><span style="color: #3d85c6;">Pieter-Jan Kindermans</span>, Kristof T. Schütt, Maximilian Alber, Klaus-Robert Müller, <span style="color: #3d85c6;">Dumitru Erhan</span>, <span style="color: #3d85c6;">Been Kim</span>, Sven Dähne</i><br><br><a href="https://openreview.net/pdf?id=HkwVAXyCW">Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks</a><br><i>Víctor Campos, <span style="color: #3d85c6;">Brendan Jou</span>, Xavier Giró-i-Nieto, Jordi Torres, Shih-Fu Chang</i><br><br><a href="https://openreview.net/pdf?id=HktJec1RZ">Towards Neural Phrase-based Machine Translation</a><br><i>Po-Sen Huang, <span style="color: #3d85c6;">Chong Wang</span>, Sitao Huang, <span style="color: #3d85c6;">Dengyong Zhou</span>, Li Deng</i><br><br><a href="https://openreview.net/pdf?id=BkeqO7x0-">Unsupervised Cipher Cracking Using Discrete GANs</a><br><i>Aidan N. Gomez, Sicong Huang, Ivan Zhang, Bryan M. Li, Muhammad Osama, <span style="color: #3d85c6;">Lukasz Kaiser </span></i><br><br><a href="https://openreview.net/pdf?id=rkcQFMZRb">Variational Image Compression With A Scale Hyperprior</a><br><i><span style="color: #3d85c6;">Johannes Ballé</span>,<span style="color: #3d85c6;">&nbsp;David Minnen</span>,<span style="color: #3d85c6;">&nbsp;Saurabh Singh</span>,<span style="color: #3d85c6;">&nbsp;Sung Jin Hwang</span>,<span style="color: #3d85c6;">&nbsp;Nick Johnston</span></i><br><br><b><u>Workshop Posters</u></b><br><a href="https://openreview.net/pdf?id=SJOYTK1vM">Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values</a><br><i><span style="color: #3d85c6;">Julius Adebayo</span>,<span style="color: #3d85c6;">&nbsp;Justin Gilmer</span>,<span style="color: #3d85c6;">&nbsp;Ian Goodfellow</span>,<span style="color: #3d85c6;">&nbsp;Been Kim</span></i><br><br><a href="https://openreview.net/pdf?id=ry-Se9kvG">Stoachastic Gradient Langevin Dynamics that Exploit Neural Network Structure</a><br><i><span style="color: #3d85c6;">Zachary Nado</span>,<span style="color: #3d85c6;">&nbsp;Jasper Snoek</span>, Bowen Xu, Roger Grosse, David Duvenaud, James Martens</i><br><br><a href="https://openreview.net/pdf?id=Sksy1ckvG">Towards Mixed-initiative generation of multi-channel sequential structure</a><br><i><span style="color: #3d85c6;">Anna Huang</span>, <span style="color: #3d85c6;">Sherol Chen</span>, Mark J. Nelson, <span style="color: #3d85c6;">Douglas Eck</span></i><br><br><a href="https://openreview.net/pdf?id=HkCnm-bAb">Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?</a><br><i><span style="color: #3d85c6;">Maithra Raghu</span>, <span style="color: #3d85c6;">Alex Irpan</span>, Jacob Andreas, Robert Kleinberg, <span style="color: #3d85c6;">Quoc V. Le</span>, Jon Kleinberg</i><br><br><a href="https://openreview.net/pdf?id=HyV7vdJPM">GILBO: One Metric to Measure Them All</a><br><i><span style="color: #3d85c6;">Alexander Alemi</span>,<span style="color: #3d85c6;">&nbsp;Ian Fischer</span></i><br><br><a href="https://openreview.net/pdf?id=B1pJ3dkwG">HoME: a Household Multimodal Environment</a><br><i>Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca Celotti, Florian Strub, Jean Rouat, <span style="color: #3d85c6;">Hugo Larochelle,</span> Aaron Courville</i><br><br><a href="https://openreview.net/pdf?id=ByoT9Fkvz">Learning to Learn without Labels</a><br><i><span style="color: #3d85c6;">Luke Metz</span>,<span style="color: #3d85c6;">&nbsp;Niru Maheswaranathan</span>, Brian Cheung, <span style="color: #3d85c6;">Jascha Sohl-Dickstein</span></i><br><br><a href="https://openreview.net/pdf?id=Bkyn3dJPG">Learning via Social Awareness: Improving Sketch Representations with Facial Feedback</a> <br><i><span style="color: #3d85c6;">Natasha Jaques</span>,<span style="color: #3d85c6;">&nbsp;Jesse Engel</span>,<span style="color: #3d85c6;">&nbsp;David Ha</span>,<span style="color: #3d85c6;">&nbsp;Fred Bertsch</span>, Rosalind Picard, <span style="color: #3d85c6;">Douglas Eck</span></i><br><br><a href="https://openreview.net/pdf?id=S1iiddyDG">Negative Eigenvalues of the Hessian in Deep Neural Networks </a><br><i>Guillaume Alain, <span style="color: #3d85c6;">Nicolas Le Roux</span>,<span style="color: #3d85c6;">&nbsp;Pierre-Antoine Manzagol</span></i><br><br><a href="https://openreview.net/pdf?id=ByCZsFyPf">Realistic Evaluation of Semi-Supervised Learning Algorithms</a><br><i><span style="color: #3d85c6;">Avital Oliver</span>,<span style="color: #3d85c6;">&nbsp;Augustus Odena</span>,<span style="color: #3d85c6;">&nbsp;Colin Raffel</span>,<span style="color: #3d85c6;">&nbsp;Ekin Cubuk</span>,<span style="color: #3d85c6;">&nbsp;lan Goodfellow</span></i><br><br><a href="https://openreview.net/pdf?id=rJWF0Fywf">Winner's Curse? On Pace, Progress, and Empirical Rigor </a><br><i><span style="color: #3d85c6;">D. Sculley</span>,<span style="color: #3d85c6;">&nbsp;Jasper Snoek</span>,<span style="color: #3d85c6;">&nbsp;Alex Wiltschko</span>,<span style="color: #3d85c6;">&nbsp;Ali Rahimi</span></i><br><br><a href="https://openreview.net/pdf?id=r1PsGFJPz">Meta-Learning for Batch Mode Active Learning</a><br><i>Sachin Ravi, <span style="color: #3d85c6;">Hugo Larochelle </span></i><br><br><a href="https://openreview.net/pdf?id=Sy1iIDkPM">To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression</a> <br><i>Michael Zhu, <span style="color: #3d85c6;">Suyog Gupta </span></i><br><br><a href="https://openreview.net/pdf?id=SkthlLkPf">Adversarial Spheres</a><br><i><span style="color: #3d85c6;">Justin Gilmer</span>,<span style="color: #3d85c6;">&nbsp;Luke Metz</span>,<span style="color: #3d85c6;">&nbsp;Fartash Faghri</span>,<span style="color: #3d85c6;">&nbsp;Sam Schoenholz</span>,<span style="color: #3d85c6;">&nbsp;Maithra Raghu,</span>,<span style="color: #3d85c6;">Martin Wattenberg</span>,<span style="color: #3d85c6;">&nbsp;Ian Goodfellow</span></i><br><br><a href="https://openreview.net/pdf?id=rk4QYDkwz">Clustering Meets Implicit Generative Models</a><br><i>Francesco Locatello, <span style="color: #3d85c6;">Damien Vincent</span>, Ilya Tolstikhin, Gunnar Ratsch, <span style="color: #3d85c6;">Sylvain Gelly,</span> <span style="color: #3d85c6;">Bernhard Scholkopf</span></i><br><br><a href="https://openreview.net/pdf?id=Byd-EfWCb">Decoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity Tasks</a><br><i>Vitalii Zhelezniak, Dan Busbridge, April Shen, <span style="color: #3d85c6;">Samuel L. Smith</span>, Nils Y. Hammerla</i><br><br><a href="https://openreview.net/pdf?id=Hy9xDwyPM">Learning Longer-term Dependencies in RNNs with Auxiliary Losses</a> <br><i><span style="color: #3d85c6;">Trieu Trinh</span>,<span style="color: #3d85c6;">&nbsp;Quoc Le</span>,<span style="color: #3d85c6;">&nbsp;Andrew Dai</span>,<span style="color: #3d85c6;">&nbsp;Thang Luong</span></i><br><br><a href="https://openreview.net/pdf?id=rk4Fz2e0b">Graph Partition Neural Networks for Semi-Supervised Classification</a><br><i>Alexander Gaunt, <span style="color: #3d85c6;">Danny Tarlow</span>, Marc Brockschmidt, Raquel Urtasun, Renjie Liao, Richard Zemel</i><br><br><a href="https://openreview.net/pdf?id=SkBYYyZRZ">Searching for Activation Functions</a><br><i><span style="color: #3d85c6;">Prajit Ramachandran</span>,<span style="color: #3d85c6;">&nbsp;Barret Zoph</span>,<span style="color: #3d85c6;">&nbsp;Quoc Le</span></i><br><br><a href="https://openreview.net/pdf?id=SJDJNzWAZ">Time-Dependent Representation for Neural Event Sequence Prediction</a><br><i><span style="color: #3d85c6;">Yang Li, Nan Du</span>,<span style="color: #3d85c6;">&nbsp;Samy Bengio</span></i><br><br><a href="https://openreview.net/pdf?id=ByQZjx-0-">Faster Discovery of Neural Architectures by Searching for Paths in a Large Model</a> <br><i>Hieu Pham, Melody Guan, <span style="color: #3d85c6;">Barret Zoph</span>,<span style="color: #3d85c6;">&nbsp;Quoc V. Le</span>,<span style="color: #3d85c6;">&nbsp;Jeff Dean</span></i><br><br><a href="https://openreview.net/pdf?id=rk6H0ZbRb">Intriguing Properties of Adversarial Examples</a><br><i><span style="color: #3d85c6;">Ekin Dogus Cubuk</span>,<span style="color: #3d85c6;">&nbsp;Barret Zoph</span>,<span style="color: #3d85c6;">&nbsp;Sam Schoenholz</span>,<span style="color: #3d85c6;">&nbsp;Quoc Le </span></i><br><br><a href="https://openreview.net/pdf?id=B1NT3TAIM">PPP-Net: Platform-aware Progressive Search for Pareto-optimal Neural Architectures</a><br><i>Jin-Dong Dong, An-Chieh Cheng, <span style="color: #3d85c6;">Da-Cheng Juan</span>,<span style="color: #3d85c6;">&nbsp;Wei Wei</span>, Min Sun</i><br><br><a href="https://openreview.net/pdf?id=HyL0IKJwM">The Mirage of Action-Dependent Baselines in Reinforcement Learning</a><br><i><span style="color: #3d85c6;">George Tucker</span>,<span style="color: #3d85c6;">&nbsp;Surya Bhupatiraju</span>,<span style="color: #3d85c6;">&nbsp;Shixiang Gu</span>, Richard E. Turner, Zoubin Ghahramani, <span style="color: #3d85c6;">Sergey Levine</span></i><br><br><a href="https://openreview.net/pdf?id=By3v9k-RZ">Learning to Organize Knowledge with N-Gram Machines</a><br><i>Fan Yang, <span style="color: #3d85c6;">Jiazhong Nie</span>, William W. Cohen, Ni Lao</i><br><br><a href="https://openreview.net/pdf?id=r1qKBtJvG">Online variance-reducing optimization</a><br><i><span style="color: #3d85c6;">Nicolas Le Roux</span>, Reza Babanezhad, <span style="color: #3d85c6;">Pierre-Antoine Manzagol</span></i> 
                        -->

<!-- <span itemprop="author" itemscope="itemscope" itemtype="http://schema.org/Person">
  <meta content="https://plus.google.com/116899029375914044550" itemprop="url">
</span>
                        </div>
</div>
<div class="share">
<span class="gplus-share social-wrapper" data-href="https://plus.google.com/share?url=http://ai.googleblog.com/2018/04/google-at-iclr-2018.html">
<img alt="Share on Google+" height="24" src="./blog_files/ic_w_post_gplus_black_24dp.png" width="24">
</span>
<span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Google at ICLR 2018&amp;url=http://ai.googleblog.com/2018/04/google-at-iclr-2018.html&amp;via=googleresearch">
<img alt="Share on Twitter" height="24" src="./blog_files/post_twitter_black_24dp.png" width="24">
</span>
<span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2018/04/google-at-iclr-2018.html">
<img alt="Share on Facebook" height="24" src="./blog_files/post_facebook_black_24dp.png" width="24">
</span>
</div>
<div class="comment-container"> -->
<i class="comment-img material-icons">
                            👍
                          </i>

</div>
</body></html>